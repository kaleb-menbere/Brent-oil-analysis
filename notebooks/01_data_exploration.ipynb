{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Task 1 - Data Exploration Script\n",
    "Loads and explores the Brent Oil Price dataset to understand schema, content, and relationships.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main exploration function\"\"\"\n",
    "    \n",
    "    # ====== 1. Configuration ======\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BRENT OIL PRICE DATA EXPLORATION SCRIPT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    DATA_DIR = Path(\"../data/raw/\")\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    if not DATA_DIR.exists():\n",
    "        print(f\"ERROR: Data directory not found at {DATA_DIR}\")\n",
    "        print(\"Please adjust the DATA_DIR path or create the data directory.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # ====== 2. Load datasets ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LOADING DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load Brent Oil Prices CSV\n",
    "        oil_file = DATA_DIR / \"BrentOilPrices.csv\"\n",
    "        \n",
    "        if oil_file.exists():\n",
    "            print(\"Loading Brent Oil Prices data...\")\n",
    "            oil_df = pd.read_csv(oil_file)\n",
    "            print(f\"✓ Loaded BrentOilPrices.csv ({len(oil_df)} rows)\")\n",
    "            \n",
    "            # For compatibility with original script structure\n",
    "            data_df = oil_df.copy()\n",
    "            impact_links_df = pd.DataFrame()  # Empty for now\n",
    "            reference_codes_df = pd.DataFrame()  # Empty for now\n",
    "        else:\n",
    "            print(f\"ERROR: Data file not found in {DATA_DIR}\")\n",
    "            print(f\"Expected: {oil_file.name}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading data: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # ====== 3. Explore schema ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA SCHEMA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n--- BRENT OIL PRICES DATASET STRUCTURE ---\")\n",
    "    print(f\"Total records: {len(data_df):,}\")\n",
    "    print(f\"Columns: {len(data_df.columns)}\")\n",
    "    print(\"\\nColumn names:\")\n",
    "    for i, col in enumerate(data_df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Data types:\")\n",
    "    print(data_df.dtypes.to_string())\n",
    "    \n",
    "    # ====== 4. Show sample data ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    print(data_df.head(10).to_string())\n",
    "    \n",
    "    print(\"\\nLast 10 rows:\")\n",
    "    print(data_df.tail(10).to_string())\n",
    "    \n",
    "    # ====== 5. Basic statistics ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BASIC STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    print(data_df.describe().to_string())\n",
    "    \n",
    "    # ====== 6. Check for missing values ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MISSING VALUE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    missing_values = data_df.isnull().sum()\n",
    "    total_cells = np.prod(data_df.shape)\n",
    "    total_missing = missing_values.sum()\n",
    "    \n",
    "    print(f\"\\nTotal cells in dataset: {total_cells:,}\")\n",
    "    print(f\"Total missing values: {total_missing:,} ({total_missing/total_cells*100:.2f}%)\")\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        print(\"\\nMissing values by column:\")\n",
    "        for column, missing_count in missing_values[missing_values > 0].items():\n",
    "            percentage = (missing_count / len(data_df)) * 100\n",
    "            print(f\"  {column:20s}: {missing_count:6,} ({percentage:.2f}%)\")\n",
    "    \n",
    "    # ====== 7. Temporal coverage analysis ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEMPORAL COVERAGE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check for date-related columns\n",
    "    date_columns = [col for col in data_df.columns if 'date' in col.lower() or 'time' in col.lower() or 'year' in col.lower() or 'month' in col.lower()]\n",
    "    \n",
    "    if date_columns:\n",
    "        print(f\"\\nDate-related columns found: {date_columns}\")\n",
    "        \n",
    "        for date_col in date_columns:\n",
    "            print(f\"\\n--- Analyzing {date_col} ---\")\n",
    "            \n",
    "            # Try to convert to datetime\n",
    "            try:\n",
    "                data_df[f'{date_col}_parsed'] = pd.to_datetime(data_df[date_col], errors='coerce')\n",
    "                valid_dates = data_df[f'{date_col}_parsed'].dropna()\n",
    "                \n",
    "                if len(valid_dates) > 0:\n",
    "                    print(f\"Valid dates: {len(valid_dates):,} ({len(valid_dates)/len(data_df)*100:.1f}%)\")\n",
    "                    print(f\"Date range: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "                    print(f\"Timespan: {(valid_dates.max() - valid_dates.min()).days / 365.25:.1f} years\")\n",
    "                    \n",
    "                    # Check for duplicates or gaps\n",
    "                    date_counts = valid_dates.value_counts().sort_values(ascending=False)\n",
    "                    duplicates = date_counts[date_counts > 1]\n",
    "                    if len(duplicates) > 0:\n",
    "                        print(f\"⚠️  {len(duplicates):,} dates have duplicate entries\")\n",
    "                    \n",
    "                    # Check for gaps\n",
    "                    date_range = pd.date_range(start=valid_dates.min(), end=valid_dates.max())\n",
    "                    missing_dates = date_range.difference(valid_dates)\n",
    "                    if len(missing_dates) > 0:\n",
    "                        print(f\"⚠️  {len(missing_dates):,} dates missing from continuous range\")\n",
    "                    \n",
    "                    # Yearly distribution\n",
    "                    print(\"\\nRecords per year:\")\n",
    "                    yearly_counts = valid_dates.dt.year.value_counts().sort_index()\n",
    "                    for year, count in yearly_counts.items():\n",
    "                        print(f\"  {year}: {count:6,}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Could not parse {date_col} as datetime: {e}\")\n",
    "    else:\n",
    "        # Try to identify any column that might be a date\n",
    "        print(\"\\nNo obvious date columns found. Checking all columns...\")\n",
    "        for col in data_df.columns:\n",
    "            sample = data_df[col].dropna().head(5).astype(str).tolist()\n",
    "            # Simple heuristic: if any sample looks like a date\n",
    "            date_like = any('/' in str(s) or '-' in str(s) for s in sample if len(str(s)) > 6)\n",
    "            if date_like:\n",
    "                print(f\"  {col} might be a date column (sample: {sample})\")\n",
    "    \n",
    "    # ====== 8. Price/Value analysis ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PRICE/VALUE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Look for columns that might contain price/value data\n",
    "    value_keywords = ['price', 'value', 'close', 'open', 'high', 'low', 'volume', 'amount']\n",
    "    value_columns = [col for col in data_df.columns \n",
    "                    if any(keyword in col.lower() for keyword in value_keywords)]\n",
    "    \n",
    "    if value_columns:\n",
    "        print(f\"\\nPrice/value-related columns found: {value_columns}\")\n",
    "        \n",
    "        for value_col in value_columns:\n",
    "            print(f\"\\n--- Analyzing {value_col} ---\")\n",
    "            \n",
    "            # Convert to numeric if possible\n",
    "            data_df[f'{value_col}_numeric'] = pd.to_numeric(data_df[value_col], errors='coerce')\n",
    "            numeric_values = data_df[f'{value_col}_numeric'].dropna()\n",
    "            \n",
    "            if len(numeric_values) > 0:\n",
    "                print(f\"Numeric values: {len(numeric_values):,} ({len(numeric_values)/len(data_df)*100:.1f}%)\")\n",
    "                print(f\"Min: {numeric_values.min():.2f}\")\n",
    "                print(f\"Max: {numeric_values.max():.2f}\")\n",
    "                print(f\"Mean: {numeric_values.mean():.2f}\")\n",
    "                print(f\"Median: {numeric_values.median():.2f}\")\n",
    "                print(f\"Std Dev: {numeric_values.std():.2f}\")\n",
    "                \n",
    "                # Check for outliers (using IQR method)\n",
    "                Q1 = numeric_values.quantile(0.25)\n",
    "                Q3 = numeric_values.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                outliers = numeric_values[(numeric_values < lower_bound) | (numeric_values > upper_bound)]\n",
    "                \n",
    "                if len(outliers) > 0:\n",
    "                    print(f\"⚠️  {len(outliers):,} potential outliers found ({len(outliers)/len(numeric_values)*100:.2f}%)\")\n",
    "                \n",
    "                # Check for zero or negative values if that's unexpected\n",
    "                non_positive = numeric_values[numeric_values <= 0]\n",
    "                if len(non_positive) > 0:\n",
    "                    print(f\"⚠️  {len(non_positive):,} non-positive values found\")\n",
    "    \n",
    "    # ====== 9. Correlation analysis ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Identify numeric columns for correlation\n",
    "    numeric_cols = data_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        print(f\"\\nNumeric columns available for correlation: {numeric_cols}\")\n",
    "        \n",
    "        correlation_matrix = data_df[numeric_cols].corr()\n",
    "        \n",
    "        print(\"\\nCorrelation matrix:\")\n",
    "        print(correlation_matrix.to_string())\n",
    "        \n",
    "        # Find highly correlated pairs\n",
    "        print(\"\\nHighly correlated pairs (|r| > 0.7):\")\n",
    "        for i in range(len(numeric_cols)):\n",
    "            for j in range(i+1, len(numeric_cols)):\n",
    "                corr = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr) > 0.7:\n",
    "                    print(f\"  {numeric_cols[i]} - {numeric_cols[j]}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNot enough numeric columns for correlation analysis\")\n",
    "    \n",
    "    # ====== 10. Data quality checks ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA QUALITY CHECKS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. Duplicate Records:\")\n",
    "    duplicates = data_df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"⚠️  {duplicates:,} duplicate records found ({duplicates/len(data_df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate records found\")\n",
    "    \n",
    "    print(\"\\n2. Inconsistent Data Types:\")\n",
    "    for col in data_df.columns:\n",
    "        unique_types = data_df[col].apply(type).nunique()\n",
    "        if unique_types > 1:\n",
    "            print(f\"⚠️  {col} has {unique_types} different data types\")\n",
    "    \n",
    "    print(\"\\n3. Unusual Values:\")\n",
    "    # Check for extreme values in numeric columns\n",
    "    for col in numeric_cols:\n",
    "        if len(data_df[col].dropna()) > 0:\n",
    "            mean_val = data_df[col].mean()\n",
    "            std_val = data_df[col].std()\n",
    "            extreme_threshold = mean_val + 5 * std_val\n",
    "            extreme_values = data_df[data_df[col] > extreme_threshold]\n",
    "            if len(extreme_values) > 0:\n",
    "                print(f\"⚠️  {col} has {len(extreme_values)} values > 5 std dev from mean\")\n",
    "    \n",
    "    # ====== 11. Save summary outputs ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING SUMMARY OUTPUTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary_stats = {\n",
    "        \"total_records\": len(data_df),\n",
    "        \"total_columns\": len(data_df.columns),\n",
    "        \"total_missing_values\": total_missing,\n",
    "        \"missing_percentage\": total_missing/total_cells*100,\n",
    "        \"duplicate_records\": duplicates,\n",
    "        \"date_columns_found\": len(date_columns),\n",
    "        \"numeric_columns_found\": len(numeric_cols),\n",
    "        \"value_columns_found\": len(value_columns)\n",
    "    }\n",
    "    \n",
    "    # Add column-specific info\n",
    "    for col in data_df.columns:\n",
    "        summary_stats[f\"{col}_dtype\"] = str(data_df[col].dtype)\n",
    "        summary_stats[f\"{col}_unique\"] = data_df[col].nunique()\n",
    "        summary_stats[f\"{col}_missing\"] = data_df[col].isnull().sum()\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame.from_dict(\n",
    "        summary_stats, \n",
    "        orient='index', \n",
    "        columns=['value']\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_dir = Path(\"../results/reports/\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = output_dir / \"task1_oil_data_summary.csv\"\n",
    "    summary_df.to_csv(output_file)\n",
    "    print(f\"\\n✓ Summary statistics saved to: {output_file}\")\n",
    "    \n",
    "    # Save data samples\n",
    "    sample_file = output_dir / \"task1_data_samples.csv\"\n",
    "    pd.concat([data_df.head(20), data_df.tail(20)]).to_csv(sample_file, index=False)\n",
    "    print(f\"✓ Data samples saved to: {sample_file}\")\n",
    "    \n",
    "    # Save missing values report\n",
    "    if total_missing > 0:\n",
    "        missing_report = pd.DataFrame({\n",
    "            'column': missing_values.index,\n",
    "            'missing_count': missing_values.values,\n",
    "            'missing_percentage': (missing_values.values / len(data_df)) * 100\n",
    "        })\n",
    "        missing_report = missing_report[missing_report['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "        missing_file = output_dir / \"task1_missing_values_report.csv\"\n",
    "        missing_report.to_csv(missing_file, index=False)\n",
    "        print(f\"✓ Missing values report saved to: {missing_file}\")\n",
    "    \n",
    "    # ====== 12. Key findings summary ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY FINDINGS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. DATASET OVERVIEW:\")\n",
    "    print(f\"   • {summary_stats['total_records']:,} total records\")\n",
    "    print(f\"   • {summary_stats['total_columns']:,} columns\")\n",
    "    \n",
    "    print(\"\\n2. DATA QUALITY:\")\n",
    "    print(f\"   • {summary_stats['total_missing_values']:,} missing values ({summary_stats['missing_percentage']:.2f}%)\")\n",
    "    print(f\"   • {summary_stats['duplicate_records']:,} duplicate records\")\n",
    "    \n",
    "    print(\"\\n3. TEMPORAL COVERAGE:\")\n",
    "    if date_columns:\n",
    "        print(f\"   • {len(date_columns)} date-related column(s) found\")\n",
    "        # Add specific date info if available\n",
    "        for date_col in date_columns:\n",
    "            if f'{date_col}_parsed' in data_df.columns:\n",
    "                valid_dates = data_df[f'{date_col}_parsed'].dropna()\n",
    "                if len(valid_dates) > 0:\n",
    "                    print(f\"   • {date_col}: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "    else:\n",
    "        print(\"   • No obvious date columns identified\")\n",
    "    \n",
    "    print(\"\\n4. PRICE/VALUE DATA:\")\n",
    "    if value_columns:\n",
    "        print(f\"   • {len(value_columns)} value-related column(s) found\")\n",
    "        for value_col in value_columns:\n",
    "            print(f\"   • {value_col}: available for analysis\")\n",
    "    else:\n",
    "        print(\"   • No obvious price/value columns identified\")\n",
    "    \n",
    "    print(\"\\n5. FORECASTING POTENTIAL:\")\n",
    "    if date_columns and value_columns:\n",
    "        print(\"   ✓ Dataset appears suitable for time series forecasting\")\n",
    "        print(\"   • Contains both temporal and value dimensions\")\n",
    "    else:\n",
    "        print(\"   ⚠️  Dataset may need preprocessing for time series analysis\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPLORATION COMPLETE - Ready for Task 1 README\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Print next steps\n",
    "    print(\"\\nNEXT STEPS:\")\n",
    "    print(\"1. Review the generated reports in ../results/reports/\")\n",
    "    print(\"2. Write Task 1 README based on these findings\")\n",
    "    print(\"3. Consider what data enrichment might be needed\")\n",
    "    print(\"4. Plan preprocessing steps for time series analysis\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e662d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
