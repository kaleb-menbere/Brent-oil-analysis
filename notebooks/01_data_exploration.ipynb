{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7d6372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BRENT OIL PRICE DATA EXPLORATION SCRIPT\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "LOADING DATASETS\n",
      "================================================================================\n",
      "Loading Brent Oil Prices data...\n",
      "✓ Loaded BrentOilPrices.csv (9011 rows)\n",
      "\n",
      "================================================================================\n",
      "DATA SCHEMA\n",
      "================================================================================\n",
      "\n",
      "--- BRENT OIL PRICES DATASET STRUCTURE ---\n",
      "Total records: 9,011\n",
      "Columns: 2\n",
      "\n",
      "Column names:\n",
      "   1. Date\n",
      "   2. Price\n",
      "\n",
      "----------------------------------------\n",
      "Data types:\n",
      "Date         str\n",
      "Price    float64\n",
      "\n",
      "================================================================================\n",
      "SAMPLE DATA\n",
      "================================================================================\n",
      "\n",
      "First 10 rows:\n",
      "        Date  Price\n",
      "0  20-May-87  18.63\n",
      "1  21-May-87  18.45\n",
      "2  22-May-87  18.55\n",
      "3  25-May-87  18.60\n",
      "4  26-May-87  18.63\n",
      "5  27-May-87  18.60\n",
      "6  28-May-87  18.60\n",
      "7  29-May-87  18.58\n",
      "8  01-Jun-87  18.65\n",
      "9  02-Jun-87  18.68\n",
      "\n",
      "Last 10 rows:\n",
      "              Date  Price\n",
      "9001  Nov 01, 2022  95.12\n",
      "9002  Nov 02, 2022  96.07\n",
      "9003  Nov 03, 2022  95.29\n",
      "9004  Nov 04, 2022  99.53\n",
      "9005  Nov 07, 2022  99.87\n",
      "9006  Nov 08, 2022  96.85\n",
      "9007  Nov 09, 2022  93.05\n",
      "9008  Nov 10, 2022  94.25\n",
      "9009  Nov 11, 2022  96.37\n",
      "9010  Nov 14, 2022  93.59\n",
      "\n",
      "================================================================================\n",
      "BASIC STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Descriptive statistics:\n",
      "             Price\n",
      "count  9011.000000\n",
      "mean     48.420782\n",
      "std      32.860110\n",
      "min       9.100000\n",
      "25%      19.050000\n",
      "50%      38.570000\n",
      "75%      70.090000\n",
      "max     143.950000\n",
      "\n",
      "================================================================================\n",
      "MISSING VALUE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total cells in dataset: 18,022\n",
      "Total missing values: 0 (0.00%)\n",
      "\n",
      "================================================================================\n",
      "TEMPORAL COVERAGE\n",
      "================================================================================\n",
      "\n",
      "Date-related columns found: ['Date']\n",
      "\n",
      "--- Analyzing Date ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\321\\AppData\\Local\\Temp\\ipykernel_15272\\1020993614.py:124: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_df[f'{date_col}_parsed'] = pd.to_datetime(data_df[date_col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid dates: 9,011 (100.0%)\n",
      "Date range: 1987-05-20 to 2022-11-14\n",
      "Timespan: 35.5 years\n",
      "⚠️  3,952 dates missing from continuous range\n",
      "\n",
      "Records per year:\n",
      "  1987:    160\n",
      "  1988:    255\n",
      "  1989:    254\n",
      "  1990:    256\n",
      "  1991:    257\n",
      "  1992:    257\n",
      "  1993:    252\n",
      "  1994:    252\n",
      "  1995:    253\n",
      "  1996:    254\n",
      "  1997:    248\n",
      "  1998:    253\n",
      "  1999:    249\n",
      "  2000:    253\n",
      "  2001:    257\n",
      "  2002:    255\n",
      "  2003:    258\n",
      "  2004:    261\n",
      "  2005:    257\n",
      "  2006:    255\n",
      "  2007:    250\n",
      "  2008:    253\n",
      "  2009:    252\n",
      "  2010:    252\n",
      "  2011:    248\n",
      "  2012:    249\n",
      "  2013:    252\n",
      "  2014:    254\n",
      "  2015:    255\n",
      "  2016:    255\n",
      "  2017:    256\n",
      "  2018:    252\n",
      "  2019:    258\n",
      "  2020:    256\n",
      "  2021:    253\n",
      "  2022:    220\n",
      "\n",
      "================================================================================\n",
      "PRICE/VALUE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Price/value-related columns found: ['Price']\n",
      "\n",
      "--- Analyzing Price ---\n",
      "Numeric values: 9,011 (100.0%)\n",
      "Min: 9.10\n",
      "Max: 143.95\n",
      "Mean: 48.42\n",
      "Median: 38.57\n",
      "Std Dev: 32.86\n",
      "\n",
      "================================================================================\n",
      "CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Numeric columns available for correlation: ['Price', 'Price_numeric']\n",
      "\n",
      "Correlation matrix:\n",
      "               Price  Price_numeric\n",
      "Price            1.0            1.0\n",
      "Price_numeric    1.0            1.0\n",
      "\n",
      "Highly correlated pairs (|r| > 0.7):\n",
      "  Price - Price_numeric: 1.000\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "1. Duplicate Records:\n",
      "✓ No duplicate records found\n",
      "\n",
      "2. Inconsistent Data Types:\n",
      "\n",
      "3. Unusual Values:\n",
      "\n",
      "================================================================================\n",
      "SAVING SUMMARY OUTPUTS\n",
      "================================================================================\n",
      "\n",
      "✓ Summary statistics saved to: ..\\results\\reports\\task1_oil_data_summary.csv\n",
      "✓ Data samples saved to: ..\\results\\reports\\task1_data_samples.csv\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. DATASET OVERVIEW:\n",
      "   • 9,011 total records\n",
      "   • 4 columns\n",
      "\n",
      "2. DATA QUALITY:\n",
      "   • 0 missing values (0.00%)\n",
      "   • 0 duplicate records\n",
      "\n",
      "3. TEMPORAL COVERAGE:\n",
      "   • 1 date-related column(s) found\n",
      "   • Date: 1987-05-20 to 2022-11-14\n",
      "\n",
      "4. PRICE/VALUE DATA:\n",
      "   • 1 value-related column(s) found\n",
      "   • Price: available for analysis\n",
      "\n",
      "5. FORECASTING POTENTIAL:\n",
      "   ✓ Dataset appears suitable for time series forecasting\n",
      "   • Contains both temporal and value dimensions\n",
      "\n",
      "================================================================================\n",
      "EXPLORATION COMPLETE - Ready for Task 1 README\n",
      "================================================================================\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Review the generated reports in ../results/reports/\n",
      "2. Write Task 1 README based on these findings\n",
      "3. Consider what data enrichment might be needed\n",
      "4. Plan preprocessing steps for time series analysis\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Task 1 - Data Exploration Script\n",
    "Loads and explores the Brent Oil Price dataset to understand schema, content, and relationships.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main exploration function\"\"\"\n",
    "    \n",
    "    # ====== 1. Configuration ======\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BRENT OIL PRICE DATA EXPLORATION SCRIPT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    DATA_DIR = Path(\"../data/raw/\")\n",
    "    \n",
    "    # Check if data directory exists\n",
    "    if not DATA_DIR.exists():\n",
    "        print(f\"ERROR: Data directory not found at {DATA_DIR}\")\n",
    "        print(\"Please adjust the DATA_DIR path or create the data directory.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # ====== 2. Load datasets ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LOADING DATASETS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load Brent Oil Prices CSV\n",
    "        oil_file = DATA_DIR / \"BrentOilPrices.csv\"\n",
    "        \n",
    "        if oil_file.exists():\n",
    "            print(\"Loading Brent Oil Prices data...\")\n",
    "            oil_df = pd.read_csv(oil_file)\n",
    "            print(f\"✓ Loaded BrentOilPrices.csv ({len(oil_df)} rows)\")\n",
    "            \n",
    "            # For compatibility with original script structure\n",
    "            data_df = oil_df.copy()\n",
    "            impact_links_df = pd.DataFrame()  # Empty for now\n",
    "            reference_codes_df = pd.DataFrame()  # Empty for now\n",
    "        else:\n",
    "            print(f\"ERROR: Data file not found in {DATA_DIR}\")\n",
    "            print(f\"Expected: {oil_file.name}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading data: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # ====== 3. Explore schema ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA SCHEMA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n--- BRENT OIL PRICES DATASET STRUCTURE ---\")\n",
    "    print(f\"Total records: {len(data_df):,}\")\n",
    "    print(f\"Columns: {len(data_df.columns)}\")\n",
    "    print(\"\\nColumn names:\")\n",
    "    for i, col in enumerate(data_df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Data types:\")\n",
    "    print(data_df.dtypes.to_string())\n",
    "    \n",
    "    # ====== 4. Show sample data ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    print(data_df.head(10).to_string())\n",
    "    \n",
    "    print(\"\\nLast 10 rows:\")\n",
    "    print(data_df.tail(10).to_string())\n",
    "    \n",
    "    # ====== 5. Basic statistics ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BASIC STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    print(data_df.describe().to_string())\n",
    "    \n",
    "    # ====== 6. Check for missing values ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MISSING VALUE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    missing_values = data_df.isnull().sum()\n",
    "    total_cells = np.prod(data_df.shape)\n",
    "    total_missing = missing_values.sum()\n",
    "    \n",
    "    print(f\"\\nTotal cells in dataset: {total_cells:,}\")\n",
    "    print(f\"Total missing values: {total_missing:,} ({total_missing/total_cells*100:.2f}%)\")\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        print(\"\\nMissing values by column:\")\n",
    "        for column, missing_count in missing_values[missing_values > 0].items():\n",
    "            percentage = (missing_count / len(data_df)) * 100\n",
    "            print(f\"  {column:20s}: {missing_count:6,} ({percentage:.2f}%)\")\n",
    "    \n",
    "    # ====== 7. Temporal coverage analysis ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEMPORAL COVERAGE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check for date-related columns\n",
    "    date_columns = [col for col in data_df.columns if 'date' in col.lower() or 'time' in col.lower() or 'year' in col.lower() or 'month' in col.lower()]\n",
    "    \n",
    "    if date_columns:\n",
    "        print(f\"\\nDate-related columns found: {date_columns}\")\n",
    "        \n",
    "        for date_col in date_columns:\n",
    "            print(f\"\\n--- Analyzing {date_col} ---\")\n",
    "            \n",
    "            # Try to convert to datetime\n",
    "            try:\n",
    "                data_df[f'{date_col}_parsed'] = pd.to_datetime(data_df[date_col], errors='coerce')\n",
    "                valid_dates = data_df[f'{date_col}_parsed'].dropna()\n",
    "                \n",
    "                if len(valid_dates) > 0:\n",
    "                    print(f\"Valid dates: {len(valid_dates):,} ({len(valid_dates)/len(data_df)*100:.1f}%)\")\n",
    "                    print(f\"Date range: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "                    print(f\"Timespan: {(valid_dates.max() - valid_dates.min()).days / 365.25:.1f} years\")\n",
    "                    \n",
    "                    # Check for duplicates or gaps\n",
    "                    date_counts = valid_dates.value_counts().sort_values(ascending=False)\n",
    "                    duplicates = date_counts[date_counts > 1]\n",
    "                    if len(duplicates) > 0:\n",
    "                        print(f\"⚠️  {len(duplicates):,} dates have duplicate entries\")\n",
    "                    \n",
    "                    # Check for gaps\n",
    "                    date_range = pd.date_range(start=valid_dates.min(), end=valid_dates.max())\n",
    "                    missing_dates = date_range.difference(valid_dates)\n",
    "                    if len(missing_dates) > 0:\n",
    "                        print(f\"⚠️  {len(missing_dates):,} dates missing from continuous range\")\n",
    "                    \n",
    "                    # Yearly distribution\n",
    "                    print(\"\\nRecords per year:\")\n",
    "                    yearly_counts = valid_dates.dt.year.value_counts().sort_index()\n",
    "                    for year, count in yearly_counts.items():\n",
    "                        print(f\"  {year}: {count:6,}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Could not parse {date_col} as datetime: {e}\")\n",
    "    else:\n",
    "        # Try to identify any column that might be a date\n",
    "        print(\"\\nNo obvious date columns found. Checking all columns...\")\n",
    "        for col in data_df.columns:\n",
    "            sample = data_df[col].dropna().head(5).astype(str).tolist()\n",
    "            # Simple heuristic: if any sample looks like a date\n",
    "            date_like = any('/' in str(s) or '-' in str(s) for s in sample if len(str(s)) > 6)\n",
    "            if date_like:\n",
    "                print(f\"  {col} might be a date column (sample: {sample})\")\n",
    "    \n",
    "    # ====== 8. Price/Value analysis ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PRICE/VALUE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Look for columns that might contain price/value data\n",
    "    value_keywords = ['price', 'value', 'close', 'open', 'high', 'low', 'volume', 'amount']\n",
    "    value_columns = [col for col in data_df.columns \n",
    "                    if any(keyword in col.lower() for keyword in value_keywords)]\n",
    "    \n",
    "    if value_columns:\n",
    "        print(f\"\\nPrice/value-related columns found: {value_columns}\")\n",
    "        \n",
    "        for value_col in value_columns:\n",
    "            print(f\"\\n--- Analyzing {value_col} ---\")\n",
    "            \n",
    "            # Convert to numeric if possible\n",
    "            data_df[f'{value_col}_numeric'] = pd.to_numeric(data_df[value_col], errors='coerce')\n",
    "            numeric_values = data_df[f'{value_col}_numeric'].dropna()\n",
    "            \n",
    "            if len(numeric_values) > 0:\n",
    "                print(f\"Numeric values: {len(numeric_values):,} ({len(numeric_values)/len(data_df)*100:.1f}%)\")\n",
    "                print(f\"Min: {numeric_values.min():.2f}\")\n",
    "                print(f\"Max: {numeric_values.max():.2f}\")\n",
    "                print(f\"Mean: {numeric_values.mean():.2f}\")\n",
    "                print(f\"Median: {numeric_values.median():.2f}\")\n",
    "                print(f\"Std Dev: {numeric_values.std():.2f}\")\n",
    "                \n",
    "                # Check for outliers (using IQR method)\n",
    "                Q1 = numeric_values.quantile(0.25)\n",
    "                Q3 = numeric_values.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                outliers = numeric_values[(numeric_values < lower_bound) | (numeric_values > upper_bound)]\n",
    "                \n",
    "                if len(outliers) > 0:\n",
    "                    print(f\"⚠️  {len(outliers):,} potential outliers found ({len(outliers)/len(numeric_values)*100:.2f}%)\")\n",
    "                \n",
    "                # Check for zero or negative values if that's unexpected\n",
    "                non_positive = numeric_values[numeric_values <= 0]\n",
    "                if len(non_positive) > 0:\n",
    "                    print(f\"⚠️  {len(non_positive):,} non-positive values found\")\n",
    "    \n",
    "    # ====== 9. Correlation analysis ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Identify numeric columns for correlation\n",
    "    numeric_cols = data_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_cols) > 1:\n",
    "        print(f\"\\nNumeric columns available for correlation: {numeric_cols}\")\n",
    "        \n",
    "        correlation_matrix = data_df[numeric_cols].corr()\n",
    "        \n",
    "        print(\"\\nCorrelation matrix:\")\n",
    "        print(correlation_matrix.to_string())\n",
    "        \n",
    "        # Find highly correlated pairs\n",
    "        print(\"\\nHighly correlated pairs (|r| > 0.7):\")\n",
    "        for i in range(len(numeric_cols)):\n",
    "            for j in range(i+1, len(numeric_cols)):\n",
    "                corr = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr) > 0.7:\n",
    "                    print(f\"  {numeric_cols[i]} - {numeric_cols[j]}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNot enough numeric columns for correlation analysis\")\n",
    "    \n",
    "    # ====== 10. Data quality checks ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA QUALITY CHECKS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. Duplicate Records:\")\n",
    "    duplicates = data_df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"⚠️  {duplicates:,} duplicate records found ({duplicates/len(data_df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate records found\")\n",
    "    \n",
    "    print(\"\\n2. Inconsistent Data Types:\")\n",
    "    for col in data_df.columns:\n",
    "        unique_types = data_df[col].apply(type).nunique()\n",
    "        if unique_types > 1:\n",
    "            print(f\"⚠️  {col} has {unique_types} different data types\")\n",
    "    \n",
    "    print(\"\\n3. Unusual Values:\")\n",
    "    # Check for extreme values in numeric columns\n",
    "    for col in numeric_cols:\n",
    "        if len(data_df[col].dropna()) > 0:\n",
    "            mean_val = data_df[col].mean()\n",
    "            std_val = data_df[col].std()\n",
    "            extreme_threshold = mean_val + 5 * std_val\n",
    "            extreme_values = data_df[data_df[col] > extreme_threshold]\n",
    "            if len(extreme_values) > 0:\n",
    "                print(f\"⚠️  {col} has {len(extreme_values)} values > 5 std dev from mean\")\n",
    "    \n",
    "    # ====== 11. Save summary outputs ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING SUMMARY OUTPUTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary_stats = {\n",
    "        \"total_records\": len(data_df),\n",
    "        \"total_columns\": len(data_df.columns),\n",
    "        \"total_missing_values\": total_missing,\n",
    "        \"missing_percentage\": total_missing/total_cells*100,\n",
    "        \"duplicate_records\": duplicates,\n",
    "        \"date_columns_found\": len(date_columns),\n",
    "        \"numeric_columns_found\": len(numeric_cols),\n",
    "        \"value_columns_found\": len(value_columns)\n",
    "    }\n",
    "    \n",
    "    # Add column-specific info\n",
    "    for col in data_df.columns:\n",
    "        summary_stats[f\"{col}_dtype\"] = str(data_df[col].dtype)\n",
    "        summary_stats[f\"{col}_unique\"] = data_df[col].nunique()\n",
    "        summary_stats[f\"{col}_missing\"] = data_df[col].isnull().sum()\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame.from_dict(\n",
    "        summary_stats, \n",
    "        orient='index', \n",
    "        columns=['value']\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_dir = Path(\"../results/reports/\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = output_dir / \"task1_oil_data_summary.csv\"\n",
    "    summary_df.to_csv(output_file)\n",
    "    print(f\"\\n✓ Summary statistics saved to: {output_file}\")\n",
    "    \n",
    "    # Save data samples\n",
    "    sample_file = output_dir / \"task1_data_samples.csv\"\n",
    "    pd.concat([data_df.head(20), data_df.tail(20)]).to_csv(sample_file, index=False)\n",
    "    print(f\"✓ Data samples saved to: {sample_file}\")\n",
    "    \n",
    "    # Save missing values report\n",
    "    if total_missing > 0:\n",
    "        missing_report = pd.DataFrame({\n",
    "            'column': missing_values.index,\n",
    "            'missing_count': missing_values.values,\n",
    "            'missing_percentage': (missing_values.values / len(data_df)) * 100\n",
    "        })\n",
    "        missing_report = missing_report[missing_report['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "        missing_file = output_dir / \"task1_missing_values_report.csv\"\n",
    "        missing_report.to_csv(missing_file, index=False)\n",
    "        print(f\"✓ Missing values report saved to: {missing_file}\")\n",
    "    \n",
    "    # ====== 12. Key findings summary ======\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY FINDINGS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. DATASET OVERVIEW:\")\n",
    "    print(f\"   • {summary_stats['total_records']:,} total records\")\n",
    "    print(f\"   • {summary_stats['total_columns']:,} columns\")\n",
    "    \n",
    "    print(\"\\n2. DATA QUALITY:\")\n",
    "    print(f\"   • {summary_stats['total_missing_values']:,} missing values ({summary_stats['missing_percentage']:.2f}%)\")\n",
    "    print(f\"   • {summary_stats['duplicate_records']:,} duplicate records\")\n",
    "    \n",
    "    print(\"\\n3. TEMPORAL COVERAGE:\")\n",
    "    if date_columns:\n",
    "        print(f\"   • {len(date_columns)} date-related column(s) found\")\n",
    "        # Add specific date info if available\n",
    "        for date_col in date_columns:\n",
    "            if f'{date_col}_parsed' in data_df.columns:\n",
    "                valid_dates = data_df[f'{date_col}_parsed'].dropna()\n",
    "                if len(valid_dates) > 0:\n",
    "                    print(f\"   • {date_col}: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "    else:\n",
    "        print(\"   • No obvious date columns identified\")\n",
    "    \n",
    "    print(\"\\n4. PRICE/VALUE DATA:\")\n",
    "    if value_columns:\n",
    "        print(f\"   • {len(value_columns)} value-related column(s) found\")\n",
    "        for value_col in value_columns:\n",
    "            print(f\"   • {value_col}: available for analysis\")\n",
    "    else:\n",
    "        print(\"   • No obvious price/value columns identified\")\n",
    "    \n",
    "    print(\"\\n5. FORECASTING POTENTIAL:\")\n",
    "    if date_columns and value_columns:\n",
    "        print(\"   ✓ Dataset appears suitable for time series forecasting\")\n",
    "        print(\"   • Contains both temporal and value dimensions\")\n",
    "    else:\n",
    "        print(\"   ⚠️  Dataset may need preprocessing for time series analysis\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPLORATION COMPLETE - Ready for Task 1 README\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Print next steps\n",
    "    print(\"\\nNEXT STEPS:\")\n",
    "    print(\"1. Review the generated reports in ../results/reports/\")\n",
    "    print(\"2. Write Task 1 README based on these findings\")\n",
    "    print(\"3. Consider what data enrichment might be needed\")\n",
    "    print(\"4. Plan preprocessing steps for time series analysis\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e662d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
