{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abd781",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01marviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maz\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymc'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Task 2 - Bayesian Change Point Modeling and Insight Generation\n",
    "Apply Bayesian change point detection to identify structural breaks in Brent oil prices.\n",
    "Addresses issues found in Task 1: date parsing, missing dates, and data preparation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Handle optional imports\n",
    "try:\n",
    "    import pymc as pm\n",
    "    HAS_PYMC = True\n",
    "except ImportError:\n",
    "    print(\"Warning: PyMC not installed. Installing with: pip install pymc\")\n",
    "    HAS_PYMC = False\n",
    "\n",
    "try:\n",
    "    import arviz as az\n",
    "    HAS_ARVIZ = True\n",
    "except ImportError:\n",
    "    print(\"Warning: ArviZ not installed. Installing with: pip install arviz\")\n",
    "    HAS_ARVIZ = False\n",
    "\n",
    "try:\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    HAS_STATSMODELS = True\n",
    "except ImportError:\n",
    "    print(\"Warning: statsmodels not installed. Installing with: pip install statsmodels\")\n",
    "    HAS_STATSMODELS = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class BrentOilChangePointAnalyzer:\n",
    "    \"\"\"Class for Bayesian change point analysis on Brent oil price data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=\"../data/raw/BrentOilPrices.csv\"):\n",
    "        \"\"\"Initialize the analyzer with raw Brent oil data\"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.results_dir = Path(\"../results/change_point_analysis/\")\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.analysis_log = []\n",
    "        \n",
    "        # Check for critical dependencies\n",
    "        if not HAS_PYMC:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"CRITICAL: PyMC is required for Bayesian change point analysis.\")\n",
    "            print(\"Please install it using: pip install pymc\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "    def log_analysis(self, step, details):\n",
    "        \"\"\"Log analysis steps\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"step\": step,\n",
    "            \"details\": details\n",
    "        }\n",
    "        self.analysis_log.append(log_entry)\n",
    "        print(f\"[{timestamp}] {step}: {details}\")\n",
    "        \n",
    "    def load_and_clean_data(self):\n",
    "        \"\"\"Load and clean Brent oil price data - FIXES TASK 1 ISSUES\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DATA LOADING AND CLEANING (Fixing Task 1 Issues)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Load raw data\n",
    "        if not self.data_path.exists():\n",
    "            # Try alternative path\n",
    "            alt_path = Path(\"data/raw/BrentOilPrices.csv\")\n",
    "            if alt_path.exists():\n",
    "                self.data_path = alt_path\n",
    "            else:\n",
    "                # Try current directory\n",
    "                alt_path = Path(\"BrentOilPrices.csv\")\n",
    "                if alt_path.exists():\n",
    "                    self.data_path = alt_path\n",
    "                else:\n",
    "                    raise FileNotFoundError(\n",
    "                        f\"Data file not found. Tried:\\n\"\n",
    "                        f\"1. {self.data_path}\\n\"\n",
    "                        f\"2. {Path('data/raw/BrentOilPrices.csv')}\\n\"\n",
    "                        f\"3. {Path('BrentOilPrices.csv')}\"\n",
    "                    )\n",
    "        \n",
    "        self.raw_df = pd.read_csv(self.data_path)\n",
    "        self.log_analysis(\"Data Loading\", \n",
    "                         f\"Loaded {len(self.raw_df)} records from {self.data_path.name}\")\n",
    "        \n",
    "        print(f\"Raw data shape: {self.raw_df.shape}\")\n",
    "        print(f\"Columns: {list(self.raw_df.columns)}\")\n",
    "        \n",
    "        # Check column names\n",
    "        if len(self.raw_df.columns) >= 2:\n",
    "            # Standardize column names\n",
    "            self.raw_df.columns = ['date', 'price']\n",
    "        elif 'Price' in self.raw_df.columns and 'Date' in self.raw_df.columns:\n",
    "            self.raw_df = self.raw_df[['Date', 'Price']].copy()\n",
    "            self.raw_df.columns = ['date', 'price']\n",
    "        else:\n",
    "            print(\"Warning: Unexpected column names. Using first two columns as date and price.\")\n",
    "            self.raw_df = self.raw_df.iloc[:, :2].copy()\n",
    "            self.raw_df.columns = ['date', 'price']\n",
    "        \n",
    "        print(f\"\\nFirst 5 rows:\")\n",
    "        print(self.raw_df.head())\n",
    "        \n",
    "        # Parse dates\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"DATE PARSING\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Try multiple parsing strategies\n",
    "        for col in ['date', 'Date', 'DATE']:\n",
    "            if col in self.raw_df.columns:\n",
    "                date_col = col\n",
    "                break\n",
    "        else:\n",
    "            date_col = self.raw_df.columns[0]\n",
    "        \n",
    "        # Try different parsing approaches\n",
    "        try:\n",
    "            # Try parsing with dayfirst=True (common for European dates)\n",
    "            self.raw_df['date_parsed'] = pd.to_datetime(self.raw_df[date_col], \n",
    "                                                       dayfirst=True, \n",
    "                                                       errors='coerce')\n",
    "            success_rate = self.raw_df['date_parsed'].notna().mean()\n",
    "            print(f\"Day-first parsing success rate: {success_rate:.1%}\")\n",
    "            \n",
    "            if success_rate < 0.9:\n",
    "                # Try without dayfirst\n",
    "                self.raw_df['date_parsed'] = pd.to_datetime(self.raw_df[date_col], \n",
    "                                                           errors='coerce')\n",
    "                success_rate = self.raw_df['date_parsed'].notna().mean()\n",
    "                print(f\"Standard parsing success rate: {success_rate:.1%}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing dates: {e}\")\n",
    "            # Use string manipulation as fallback\n",
    "            self.raw_df['date_parsed'] = pd.to_datetime(self.raw_df[date_col].astype(str), \n",
    "                                                       errors='coerce')\n",
    "        \n",
    "        # Check for successful parsing\n",
    "        if self.raw_df['date_parsed'].isna().any():\n",
    "            print(f\"Warning: {self.raw_df['date_parsed'].isna().sum()} dates failed to parse\")\n",
    "            # Drop rows with failed parsing\n",
    "            self.raw_df = self.raw_df.dropna(subset=['date_parsed'])\n",
    "        \n",
    "        self.raw_df['date'] = self.raw_df['date_parsed']\n",
    "        self.raw_df = self.raw_df.drop(columns=['date_parsed'])\n",
    "        \n",
    "        # Ensure price is numeric\n",
    "        self.raw_df['price'] = pd.to_numeric(self.raw_df['price'], errors='coerce')\n",
    "        \n",
    "        # Sort by date\n",
    "        self.raw_df = self.raw_df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Handle missing values\n",
    "        missing_prices = self.raw_df['price'].isna().sum()\n",
    "        if missing_prices > 0:\n",
    "            print(f\"Missing price values: {missing_prices}\")\n",
    "            # Interpolate missing values\n",
    "            self.raw_df['price'] = self.raw_df['price'].interpolate(method='linear')\n",
    "            print(f\"  Filled {missing_prices} missing prices using linear interpolation\")\n",
    "        \n",
    "        # Create complete time series\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"CREATING COMPLETE TIME SERIES\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Create date range\n",
    "        start_date = self.raw_df['date'].min()\n",
    "        end_date = self.raw_df['date'].max()\n",
    "        all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        \n",
    "        # Create DataFrame with all dates\n",
    "        self.df = pd.DataFrame({'date': all_dates})\n",
    "        \n",
    "        # Merge with existing data\n",
    "        self.df = pd.merge(self.df, self.raw_df, on='date', how='left')\n",
    "        \n",
    "        # Fill any remaining missing prices\n",
    "        self.df['price'] = self.df['price'].interpolate(method='linear')\n",
    "        \n",
    "        # Remove any rows with NaN prices (should be very few if any)\n",
    "        self.df = self.df.dropna(subset=['price'])\n",
    "        \n",
    "        print(f\"Complete time series created:\")\n",
    "        print(f\"  Start date: {self.df['date'].min().date()}\")\n",
    "        print(f\"  End date: {self.df['date'].max().date()}\")\n",
    "        print(f\"  Total days: {len(self.df)}\")\n",
    "        print(f\"  Missing values after cleaning: {self.df['price'].isna().sum()}\")\n",
    "        \n",
    "        # Calculate log returns\n",
    "        self.df['log_price'] = np.log(self.df['price'])\n",
    "        self.df['log_return'] = self.df['log_price'].diff()\n",
    "        \n",
    "        # Remove first row (NaN from diff)\n",
    "        self.df = self.df.dropna(subset=['log_return'])\n",
    "        \n",
    "        self.log_analysis(\"Data Cleaning\", \n",
    "                         f\"Created complete time series with {len(self.df)} records\")\n",
    "        \n",
    "        # Save cleaned data\n",
    "        cleaned_path = self.results_dir / \"brent_oil_cleaned.csv\"\n",
    "        self.df.to_csv(cleaned_path, index=False)\n",
    "        print(f\"✓ Cleaned data saved to: {cleaned_path}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def exploratory_data_analysis(self):\n",
    "        \"\"\"Perform comprehensive EDA on cleaned data\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EXPLORATORY DATA ANALYSIS (CLEANED DATA)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"  Date range: {self.df['date'].min().date()} to {self.df['date'].max().date()}\")\n",
    "        print(f\"  Total days: {len(self.df)}\")\n",
    "        print(f\"  Price range: ${self.df['price'].min():.2f} - ${self.df['price'].max():.2f}\")\n",
    "        print(f\"  Average price: ${self.df['price'].mean():.2f}\")\n",
    "        print(f\"  Median price: ${self.df['price'].median():.2f}\")\n",
    "        \n",
    "        # Create EDA plots\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Price over time\n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        ax1.plot(self.df['date'], self.df['price'], color='royalblue', linewidth=1)\n",
    "        ax1.set_title('Brent Crude Oil Price History', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Year')\n",
    "        ax1.set_ylabel('Price (USD)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Log returns distribution\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        ax2.hist(self.df['log_return'].dropna(), bins=100, density=True, \n",
    "                color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        # Add normal distribution for comparison\n",
    "        mu, sigma = self.df['log_return'].mean(), self.df['log_return'].std()\n",
    "        x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)\n",
    "        ax2.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, \n",
    "                label=f'Normal (μ={mu:.4f}, σ={sigma:.4f})')\n",
    "        \n",
    "        ax2.set_title('Distribution of Log Returns', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Log Return')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Log returns time series\n",
    "        ax3 = plt.subplot(2, 2, 3)\n",
    "        ax3.plot(self.df['date'], self.df['log_return'], color='green', \n",
    "                linewidth=0.5, alpha=0.7)\n",
    "        ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "        ax3.set_title('Daily Log Returns', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Date')\n",
    "        ax3.set_ylabel('Log Return')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Rolling statistics\n",
    "        ax4 = plt.subplot(2, 2, 4)\n",
    "        rolling_mean = self.df['price'].rolling(window=30).mean()\n",
    "        rolling_std = self.df['price'].rolling(window=30).std()\n",
    "        \n",
    "        ax4.plot(self.df['date'], self.df['price'], color='royalblue', \n",
    "                linewidth=0.5, alpha=0.5, label='Daily Price')\n",
    "        ax4.plot(self.df['date'], rolling_mean, color='red', \n",
    "                linewidth=1.5, label='30-Day Rolling Mean')\n",
    "        ax4.plot(self.df['date'], rolling_mean + rolling_std, color='orange', \n",
    "                linewidth=1, linestyle='--', label='±1 Std Dev')\n",
    "        ax4.plot(self.df['date'], rolling_mean - rolling_std, color='orange', \n",
    "                linewidth=1, linestyle='--')\n",
    "        \n",
    "        ax4.set_title('Price with Rolling Statistics', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Date')\n",
    "        ax4.set_ylabel('Price (USD)')\n",
    "        ax4.legend(fontsize=8)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        eda_path = self.results_dir / \"comprehensive_eda.png\"\n",
    "        plt.savefig(eda_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistical tests (if statsmodels is available)\n",
    "        if HAS_STATSMODELS:\n",
    "            print(f\"\\n{'='*40}\")\n",
    "            print(\"STATISTICAL TESTS\")\n",
    "            print(f\"{'='*40}\")\n",
    "            \n",
    "            # Stationarity test\n",
    "            adf_result = adfuller(self.df['log_return'].dropna())\n",
    "            print(f\"ADF Test (Log Returns):\")\n",
    "            print(f\"  Test Statistic: {adf_result[0]:.4f}\")\n",
    "            print(f\"  p-value: {adf_result[1]:.4f}\")\n",
    "            print(f\"  Stationary: {'Yes' if adf_result[1] < 0.05 else 'No'}\")\n",
    "            \n",
    "            # Autocorrelation test\n",
    "            try:\n",
    "                lb_test = acorr_ljungbox(self.df['log_return'].dropna(), lags=[10], return_df=True)\n",
    "                print(f\"\\nLjung-Box Test (Log Returns):\")\n",
    "                print(f\"  Test Statistic: {lb_test['lb_stat'].iloc[0]:.4f}\")\n",
    "                print(f\"  p-value: {lb_test['lb_pvalue'].iloc[0]:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not perform Ljung-Box test: {e}\")\n",
    "        else:\n",
    "            print(\"\\nSkipping advanced statistical tests (statsmodels not installed)\")\n",
    "        \n",
    "        # Descriptive statistics\n",
    "        print(f\"\\nDescriptive Statistics (Log Returns):\")\n",
    "        desc_stats = self.df['log_return'].describe()\n",
    "        print(f\"  Mean: {desc_stats['mean']:.6f}\")\n",
    "        print(f\"  Std Dev: {desc_stats['std']:.6f}\")\n",
    "        print(f\"  Skewness: {self.df['log_return'].skew():.4f}\")\n",
    "        print(f\"  Kurtosis: {self.df['log_return'].kurtosis():.4f}\")\n",
    "        print(f\"  Min: {desc_stats['min']:.4f}\")\n",
    "        print(f\"  Max: {desc_stats['max']:.4f}\")\n",
    "        \n",
    "        self.log_analysis(\"EDA\", \"Completed exploratory data analysis\")\n",
    "        \n",
    "        return desc_stats.to_dict()\n",
    "    \n",
    "    def build_change_point_model(self, data_type='log_return'):\n",
    "        \"\"\"Build Bayesian change point model\"\"\"\n",
    "        if not HAS_PYMC:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"ERROR: PyMC is required for change point modeling\")\n",
    "            print(\"Please install it using: pip install pymc\")\n",
    "            print(\"=\"*80)\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"BUILDING BAYESIAN CHANGE POINT MODEL\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Use log returns for stationarity\n",
    "        if data_type == 'log_return':\n",
    "            data = self.df['log_return'].values\n",
    "            data_name = \"Log Returns\"\n",
    "        else:\n",
    "            data = self.df['price'].values\n",
    "            data_name = \"Prices\"\n",
    "        \n",
    "        n = len(data)\n",
    "        \n",
    "        print(f\"Modeling {data_name} with {n} data points\")\n",
    "        print(f\"Data mean: {data.mean():.4f}, std: {data.std():.4f}\")\n",
    "        \n",
    "        # Build the model\n",
    "        with pm.Model() as cp_model:\n",
    "            # Prior for change point (uniform over time)\n",
    "            tau = pm.DiscreteUniform(\"tau\", lower=1, upper=n-1)\n",
    "            \n",
    "            # Priors for means before and after change point\n",
    "            mu1 = pm.Normal(\"mu1\", mu=0, sigma=0.1)\n",
    "            mu2 = pm.Normal(\"mu2\", mu=0, sigma=0.1)\n",
    "            \n",
    "            # Single standard deviation (simpler model)\n",
    "            sigma = pm.HalfNormal(\"sigma\", sigma=0.05)\n",
    "            \n",
    "            # Create mean array using switch\n",
    "            idx = np.arange(n)\n",
    "            mean = pm.math.switch(idx < tau, mu1, mu2)\n",
    "            \n",
    "            # Likelihood\n",
    "            likelihood = pm.Normal(\"y\", mu=mean, sigma=sigma, observed=data)\n",
    "            \n",
    "            # Prior predictive check\n",
    "            prior_checks = pm.sample_prior_predictive(samples=100, random_seed=42)\n",
    "        \n",
    "        self.cp_model = cp_model\n",
    "        self.log_analysis(\"Model Building\", f\"Built change point model for {data_name}\")\n",
    "        \n",
    "        # Visualize prior predictive\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Prior predictive samples\n",
    "        for i in range(20):  # Plot fewer for clarity\n",
    "            axes[0].plot(prior_checks.prior_predictive['y'].values[0, i, 1000:2000], \n",
    "                        alpha=0.2, color='blue', linewidth=0.5)\n",
    "        axes[0].set_title('Prior Predictive Samples (subset)', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Time')\n",
    "        axes[0].set_ylabel(data_name)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Actual data (subset for comparison)\n",
    "        axes[1].plot(data[1000:2000], color='red', linewidth=0.5)\n",
    "        axes[1].set_title(f'Actual {data_name} (same subset)', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Time')\n",
    "        axes[1].set_ylabel(data_name)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        prior_plot_path = self.results_dir / \"prior_predictive.png\"\n",
    "        plt.savefig(prior_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return cp_model\n",
    "    \n",
    "    def run_mcmc(self, model, draws=1000, tune=500, chains=2):\n",
    "        \"\"\"Run MCMC sampling\"\"\"\n",
    "        if not HAS_PYMC:\n",
    "            print(\"PyMC not installed, cannot run MCMC\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"RUNNING MCMC SAMPLING\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"Sampling parameters:\")\n",
    "        print(f\"  Draws: {draws}\")\n",
    "        print(f\"  Tune: {tune}\")\n",
    "        print(f\"  Chains: {chains}\")\n",
    "        \n",
    "        try:\n",
    "            with model:\n",
    "                # Try with NUTS sampler\n",
    "                trace = pm.sample(\n",
    "                    draws=draws,\n",
    "                    tune=tune,\n",
    "                    chains=chains,\n",
    "                    cores=1,\n",
    "                    progressbar=True,\n",
    "                    random_seed=42,\n",
    "                    target_accept=0.85\n",
    "                )\n",
    "            \n",
    "            self.log_analysis(\"MCMC Sampling\", \"Completed sampling successfully\")\n",
    "            \n",
    "            return trace\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with default sampler: {e}\")\n",
    "            print(\"Trying with Metropolis sampler...\")\n",
    "            \n",
    "            with model:\n",
    "                trace = pm.sample(\n",
    "                    draws=500,\n",
    "                    tune=250,\n",
    "                    chains=2,\n",
    "                    cores=1,\n",
    "                    progressbar=True,\n",
    "                    random_seed=42,\n",
    "                    step=pm.Metropolis()\n",
    "                )\n",
    "            \n",
    "            self.log_analysis(\"MCMC Sampling\", \"Completed sampling with Metropolis\")\n",
    "            \n",
    "            return trace\n",
    "    \n",
    "    def analyze_results(self, trace):\n",
    "        \"\"\"Analyze MCMC results and identify change points\"\"\"\n",
    "        if trace is None:\n",
    "            print(\"No trace data to analyze\")\n",
    "            return None\n",
    "        \n",
    "        if not HAS_ARVIZ:\n",
    "            print(\"ArviZ not installed for detailed analysis\")\n",
    "            # Basic analysis without ArviZ\n",
    "            tau_samples = trace['tau'].flatten()\n",
    "            mu1_samples = trace['mu1'].flatten()\n",
    "            mu2_samples = trace['mu2'].flatten()\n",
    "            \n",
    "            tau_median = int(np.median(tau_samples))\n",
    "            tau_mean = int(np.mean(tau_samples))\n",
    "            \n",
    "            print(f\"\\nBasic Change Point Analysis:\")\n",
    "            print(f\"  Median τ: {tau_median}\")\n",
    "            print(f\"  Mean τ: {tau_mean}\")\n",
    "            \n",
    "            return {\n",
    "                'tau_median': tau_median,\n",
    "                'tau_mean': tau_mean,\n",
    "                'mu1_mean': float(np.mean(mu1_samples)),\n",
    "                'mu2_mean': float(np.mean(mu2_samples))\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ANALYZING CHANGE POINT RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Check convergence\n",
    "        summary = az.summary(trace, var_names=[\"tau\", \"mu1\", \"mu2\", \"sigma\"])\n",
    "        print(\"\\nParameter Summary:\")\n",
    "        print(summary.to_string())\n",
    "        \n",
    "        # Check R-hat values\n",
    "        rhat_values = summary['r_hat']\n",
    "        converged = (rhat_values < 1.1).all()\n",
    "        \n",
    "        print(f\"\\nConvergence Diagnostics:\")\n",
    "        print(f\"  All R-hat < 1.1: {converged}\")\n",
    "        \n",
    "        # Extract and analyze change point\n",
    "        tau_samples = trace.posterior['tau'].values.flatten()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        tau_mean = int(tau_samples.mean())\n",
    "        tau_median = int(np.median(tau_samples))\n",
    "        tau_hdi = az.hdi(tau_samples, hdi_prob=0.95)\n",
    "        \n",
    "        # Convert to dates\n",
    "        cp_date = self.df['date'].iloc[tau_median]\n",
    "        \n",
    "        print(f\"\\nChange Point Analysis:\")\n",
    "        print(f\"  Most likely τ: {tau_median} (index)\")\n",
    "        print(f\"  Change point date: {cp_date.date()}\")\n",
    "        print(f\"  95% HDI for τ: [{int(tau_hdi[0])}, {int(tau_hdi[1])}]\")\n",
    "        \n",
    "        # Plot results\n",
    "        self.plot_results(trace, tau_median, tau_hdi, cp_date)\n",
    "        \n",
    "        # Quantify impact\n",
    "        results = self.quantify_impact(tau_median, trace)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, trace, tau_median, tau_hdi, cp_date):\n",
    "        \"\"\"Plot analysis results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # 1. Posterior of tau\n",
    "        ax1 = axes[0, 0]\n",
    "        tau_samples = trace.posterior['tau'].values.flatten()\n",
    "        ax1.hist(tau_samples, bins=50, density=True, alpha=0.7, \n",
    "                color='steelblue', edgecolor='black')\n",
    "        ax1.axvline(x=tau_median, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Median (τ={tau_median})')\n",
    "        ax1.set_title('Posterior Distribution of Change Point (τ)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Time Index')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Means before/after\n",
    "        ax2 = axes[0, 1]\n",
    "        mu1_samples = trace.posterior['mu1'].values.flatten()\n",
    "        mu2_samples = trace.posterior['mu2'].values.flatten()\n",
    "        \n",
    "        ax2.hist(mu1_samples, bins=50, alpha=0.7, label='Before (μ₁)', \n",
    "                density=True, color='skyblue', edgecolor='black')\n",
    "        ax2.hist(mu2_samples, bins=50, alpha=0.7, label='After (μ₂)', \n",
    "                density=True, color='salmon', edgecolor='black')\n",
    "        ax2.set_title('Posterior Distributions of Means', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Mean Log Return')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Price series with change point\n",
    "        ax3 = axes[1, 0]\n",
    "        ax3.plot(self.df['date'], self.df['price'], color='royalblue', linewidth=1)\n",
    "        ax3.axvline(x=cp_date, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.8, label='Detected Change Point')\n",
    "        \n",
    "        # Add uncertainty band\n",
    "        lower_date = self.df['date'].iloc[int(tau_hdi[0])]\n",
    "        upper_date = self.df['date'].iloc[int(tau_hdi[1])]\n",
    "        ax3.axvspan(lower_date, upper_date, alpha=0.2, color='red', \n",
    "                   label='95% HDI')\n",
    "        \n",
    "        ax3.set_title('Oil Price with Detected Change Point', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Date')\n",
    "        ax3.set_ylabel('Price (USD)')\n",
    "        ax3.legend(loc='upper left')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Log returns with change point\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.plot(self.df['date'], self.df['log_return'], color='green', \n",
    "                linewidth=0.5, alpha=0.7)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "        ax4.axvline(x=cp_date, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.8)\n",
    "        ax4.axvspan(lower_date, upper_date, alpha=0.2, color='red')\n",
    "        \n",
    "        ax4.set_title('Log Returns with Change Point', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Date')\n",
    "        ax4.set_ylabel('Log Return')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = self.results_dir / \"change_point_analysis.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def quantify_impact(self, tau_median, trace):\n",
    "        \"\"\"Quantify the impact of the change point\"\"\"\n",
    "        window_size = 30  # days before/after\n",
    "        \n",
    "        before_idx = max(0, tau_median - window_size)\n",
    "        after_idx = min(len(self.df) - 1, tau_median + window_size)\n",
    "        \n",
    "        price_before = self.df['price'].iloc[before_idx:tau_median].mean()\n",
    "        price_after = self.df['price'].iloc[tau_median:after_idx].mean()\n",
    "        \n",
    "        price_change = price_after - price_before\n",
    "        price_change_pct = (price_change / price_before) * 100\n",
    "        \n",
    "        # Log return impact\n",
    "        mu1_samples = trace.posterior['mu1'].values.flatten()\n",
    "        mu2_samples = trace.posterior['mu2'].values.flatten()\n",
    "        \n",
    "        mu1_mean = mu1_samples.mean()\n",
    "        mu2_mean = mu2_samples.mean()\n",
    "        mu_change = mu2_mean - mu1_mean\n",
    "        prob_mu2_gt_mu1 = (mu2_samples > mu1_samples).mean()\n",
    "        \n",
    "        print(f\"\\nImpact Analysis (window = ±{window_size} days):\")\n",
    "        print(f\"  Average price before CP: ${price_before:.2f}\")\n",
    "        print(f\"  Average price after CP: ${price_after:.2f}\")\n",
    "        print(f\"  Price change: ${price_change:.2f} ({price_change_pct:.1f}%)\")\n",
    "        print(f\"\\n  Mean log return before: {mu1_mean:.6f}\")\n",
    "        print(f\"  Mean log return after: {mu2_mean:.6f}\")\n",
    "        print(f\"  Change in mean: {mu_change:.6f}\")\n",
    "        print(f\"  P(after > before): {prob_mu2_gt_mu1:.3f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'change_point': {\n",
    "                'index': int(tau_median),\n",
    "                'date': self.df['date'].iloc[tau_median].date().isoformat()\n",
    "            },\n",
    "            'impact': {\n",
    "                'price_before': float(price_before),\n",
    "                'price_after': float(price_after),\n",
    "                'price_change_abs': float(price_change),\n",
    "                'price_change_pct': float(price_change_pct),\n",
    "                'mu_before': float(mu1_mean),\n",
    "                'mu_after': float(mu2_mean),\n",
    "                'mu_change': float(mu_change),\n",
    "                'prob_mu2_gt_mu1': float(prob_mu2_gt_mu1)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        results_path = self.results_dir / \"change_point_results.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\n✓ Results saved to: {results_path}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_event_analysis(self, change_point_date):\n",
    "        \"\"\"Analyze events around the change point\"\"\"\n",
    "        cp_date = pd.Timestamp(change_point_date)\n",
    "        \n",
    "        # Major historical events affecting oil prices\n",
    "        historical_events = [\n",
    "            {'date': '1990-08-02', 'name': 'Iraq invades Kuwait (Gulf War)', 'type': 'Geopolitical'},\n",
    "            {'date': '2008-09-15', 'name': 'Lehman Brothers Collapse', 'type': 'Financial'},\n",
    "            {'date': '2014-06-01', 'name': 'Oil price crash begins', 'type': 'Market'},\n",
    "            {'date': '2020-03-01', 'name': 'COVID-19 pandemic spreads', 'type': 'Demand Shock'},\n",
    "            {'date': '2022-02-24', 'name': 'Russia invades Ukraine', 'type': 'Geopolitical'}\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EVENT ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Change point date: {cp_date.date()}\")\n",
    "        \n",
    "        # Find events near the change point\n",
    "        nearby_events = []\n",
    "        for event in historical_events:\n",
    "            event_date = pd.Timestamp(event['date'])\n",
    "            days_diff = (event_date - cp_date).days\n",
    "            \n",
    "            if abs(days_diff) <= 90:  # 90-day window\n",
    "                nearby_events.append({\n",
    "                    **event,\n",
    "                    'days_from_cp': days_diff,\n",
    "                    'direction': 'before' if days_diff < 0 else 'after'\n",
    "                })\n",
    "        \n",
    "        if nearby_events:\n",
    "            print(f\"\\nEvents within 90 days of change point:\")\n",
    "            for event in nearby_events:\n",
    "                print(f\"  • {event['name']} ({event['date']}, {abs(event['days_from_cp'])} days {event['direction']})\")\n",
    "        else:\n",
    "            print(\"\\nNo major events found within 90 days\")\n",
    "        \n",
    "        return nearby_events\n",
    "    \n",
    "    def run_analysis(self):\n",
    "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"BAYESIAN CHANGE POINT ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load and clean data\n",
    "            print(\"\\nStep 1: Loading and cleaning data...\")\n",
    "            self.load_and_clean_data()\n",
    "            \n",
    "            # Step 2: EDA\n",
    "            print(\"\\nStep 2: Performing exploratory data analysis...\")\n",
    "            self.exploratory_data_analysis()\n",
    "            \n",
    "            # Step 3: Build and run model (if PyMC is available)\n",
    "            if HAS_PYMC:\n",
    "                print(\"\\nStep 3: Building Bayesian change point model...\")\n",
    "                model = self.build_change_point_model(data_type='log_return')\n",
    "                \n",
    "                print(\"\\nStep 4: Running MCMC sampling...\")\n",
    "                trace = self.run_mcmc(model, draws=1000, tune=500, chains=2)\n",
    "                \n",
    "                print(\"\\nStep 5: Analyzing results...\")\n",
    "                results = self.analyze_results(trace)\n",
    "                \n",
    "                if results and 'change_point' in results:\n",
    "                    print(\"\\nStep 6: Analyzing historical events...\")\n",
    "                    events = self.run_event_analysis(results['change_point']['date'])\n",
    "                    results['nearby_events'] = events\n",
    "                \n",
    "                print(\"\\nStep 7: Generating summary report...\")\n",
    "                self.generate_summary_report(results)\n",
    "                \n",
    "                return results\n",
    "            else:\n",
    "                print(\"\\nSkipping Bayesian modeling (PyMC not installed)\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during analysis: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def generate_summary_report(self, results):\n",
    "        \"\"\"Generate summary report\"\"\"\n",
    "        if results is None:\n",
    "            return\n",
    "        \n",
    "        report = f\"\"\"# Change Point Analysis Summary\n",
    "\n",
    "## Analysis Date\n",
    "{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Data Overview\n",
    "- Period: {self.df['date'].min().date()} to {self.df['date'].max().date()}\n",
    "- Days analyzed: {len(self.df)}\n",
    "- Average price: ${self.df['price'].mean():.2f}\n",
    "\n",
    "## Detected Change Point\n",
    "\"\"\"\n",
    "        \n",
    "        if 'change_point' in results:\n",
    "            report += f\"- Date: {results['change_point']['date']}\\n\"\n",
    "            report += f\"- Price before: ${results['impact']['price_before']:.2f}\\n\"\n",
    "            report += f\"- Price after: ${results['impact']['price_after']:.2f}\\n\"\n",
    "            report += f\"- Price change: {results['impact']['price_change_pct']:.1f}%\\n\"\n",
    "            report += f\"- Mean log return change: {results['impact']['mu_change']:.6f}\\n\"\n",
    "            report += f\"- Probability (after > before): {results['impact']['prob_mu2_gt_mu1']:.3f}\\n\"\n",
    "        \n",
    "        report_path = self.results_dir / \"summary_report.md\"\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"✓ Summary report saved to: {report_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"Starting Bayesian Change Point Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create analyzer instance\n",
    "    analyzer = BrentOilChangePointAnalyzer()\n",
    "    \n",
    "    # Run analysis\n",
    "    results = analyzer.run_analysis()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nResults saved in: results/change_point_analysis/\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ANALYSIS COMPLETED WITH LIMITED FUNCTIONALITY\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nInstall PyMC for full Bayesian analysis:\")\n",
    "        print(\"  pip install pymc arviz statsmodels\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fda4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
