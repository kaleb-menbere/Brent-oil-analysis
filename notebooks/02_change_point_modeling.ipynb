{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Task 2 - Bayesian Change Point Modeling and Insight Generation\n",
    "Apply Bayesian change point detection to identify structural breaks in Brent oil prices.\n",
    "Addresses issues found in Task 1: date parsing, missing dates, and data preparation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from dateutil import parser\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class BrentOilChangePointAnalyzer:\n",
    "    \"\"\"Class for Bayesian change point analysis on Brent oil price data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=\"../data/raw/BrentOilPrices.csv\"):\n",
    "        \"\"\"Initialize the analyzer with raw Brent oil data\"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.results_dir = Path(\"../results/change_point_analysis/\")\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.analysis_log = []\n",
    "        \n",
    "    def log_analysis(self, step, details):\n",
    "        \"\"\"Log analysis steps\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"step\": step,\n",
    "            \"details\": details\n",
    "        }\n",
    "        self.analysis_log.append(log_entry)\n",
    "        print(f\"[{timestamp}] {step}: {details}\")\n",
    "        \n",
    "    def load_and_clean_data(self):\n",
    "        \"\"\"Load and clean Brent oil price data - FIXES TASK 1 ISSUES\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DATA LOADING AND CLEANING (Fixing Task 1 Issues)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Load raw data\n",
    "        if not self.data_path.exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n",
    "        \n",
    "        self.raw_df = pd.read_csv(self.data_path)\n",
    "        self.log_analysis(\"Data Loading\", \n",
    "                         f\"Loaded {len(self.raw_df)} records from {self.data_path.name}\")\n",
    "        \n",
    "        print(f\"Raw data shape: {self.raw_df.shape}\")\n",
    "        print(f\"Columns: {list(self.raw_df.columns)}\")\n",
    "        print(f\"\\nFirst 5 rows:\")\n",
    "        print(self.raw_df.head())\n",
    "        print(f\"\\nLast 5 rows:\")\n",
    "        print(self.raw_df.tail())\n",
    "        \n",
    "        # FIX 1: Standardize column names\n",
    "        self.raw_df.columns = ['date', 'price']\n",
    "        \n",
    "        # FIX 2: Parse dates with multiple format attempts\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"DATE PARSING (Fixing parsing issues)\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Try different date parsing strategies\n",
    "        date_parsed = None\n",
    "        \n",
    "        # Strategy 1: Try common formats\n",
    "        date_formats = [\n",
    "            '%Y-%m-%d', '%d-%m-%Y', '%m-%d-%Y',\n",
    "            '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y',\n",
    "            '%Y.%m.%d', '%d.%m.%Y', '%m.%d.%Y',\n",
    "            '%b %d, %Y', '%B %d, %Y', '%d %b %Y', '%d %B %Y'\n",
    "        ]\n",
    "        \n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                parsed = pd.to_datetime(self.raw_df['date'], format=fmt, errors='coerce')\n",
    "                valid_count = parsed.notna().sum()\n",
    "                if valid_count == len(self.raw_df):\n",
    "                    date_parsed = parsed\n",
    "                    print(f\"✓ Successfully parsed all dates with format: {fmt}\")\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Strategy 2: Use dateutil parser (more flexible)\n",
    "        if date_parsed is None:\n",
    "            print(\"Using dateutil parser (flexible but slower)...\")\n",
    "            date_parsed = pd.to_datetime(self.raw_df['date'], errors='coerce')\n",
    "            valid_count = date_parsed.notna().sum()\n",
    "            print(f\"Parsed {valid_count}/{len(self.raw_df)} dates with dateutil\")\n",
    "        \n",
    "        self.raw_df['date'] = date_parsed\n",
    "        \n",
    "        # FIX 3: Handle missing dates\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"MISSING DATE HANDLING\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Sort by date\n",
    "        self.raw_df = self.raw_df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Identify missing dates\n",
    "        date_range = pd.date_range(start=self.raw_df['date'].min(), \n",
    "                                  end=self.raw_df['date'].max(), \n",
    "                                  freq='D')\n",
    "        missing_dates = date_range.difference(self.raw_df['date'])\n",
    "        \n",
    "        print(f\"Date range: {self.raw_df['date'].min().date()} to {self.raw_df['date'].max().date()}\")\n",
    "        print(f\"Total days in range: {len(date_range)}\")\n",
    "        print(f\"Records in dataset: {len(self.raw_df)}\")\n",
    "        print(f\"Missing dates: {len(missing_dates)} ({len(missing_dates)/len(date_range)*100:.1f}%)\")\n",
    "        \n",
    "        # FIX 4: Handle missing values in price\n",
    "        missing_prices = self.raw_df['price'].isna().sum()\n",
    "        if missing_prices > 0:\n",
    "            print(f\"\\nMissing price values: {missing_prices}\")\n",
    "            # Fill with forward then backward fill for time series\n",
    "            self.raw_df['price'] = self.raw_df['price'].fillna(method='ffill').fillna(method='bfill')\n",
    "            print(f\"  Filled {missing_prices} missing prices using forward/backward fill\")\n",
    "        \n",
    "        # FIX 5: Create complete time series\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"CREATING COMPLETE TIME SERIES\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Create complete date range\n",
    "        complete_dates = pd.date_range(start=self.raw_df['date'].min(), \n",
    "                                      end=self.raw_df['date'].max(), \n",
    "                                      freq='D')\n",
    "        \n",
    "        # Create new DataFrame with complete dates\n",
    "        self.df = pd.DataFrame({'date': complete_dates})\n",
    "        \n",
    "        # Merge with existing data\n",
    "        self.df = pd.merge(self.df, self.raw_df, on='date', how='left')\n",
    "        \n",
    "        # Fill missing prices with interpolation\n",
    "        self.df['price'] = self.df['price'].interpolate(method='linear')\n",
    "        \n",
    "        print(f\"Complete time series created:\")\n",
    "        print(f\"  Total days: {len(self.df)}\")\n",
    "        print(f\"  Missing prices after interpolation: {self.df['price'].isna().sum()}\")\n",
    "        \n",
    "        # Calculate log returns on complete series\n",
    "        self.df['log_price'] = np.log(self.df['price'])\n",
    "        self.df['log_return'] = self.df['log_price'].diff()\n",
    "        \n",
    "        # Remove any remaining NaN values\n",
    "        self.df = self.df.dropna(subset=['price', 'log_return'])\n",
    "        \n",
    "        self.log_analysis(\"Data Cleaning\", \n",
    "                         f\"Created complete time series with {len(self.df)} records\")\n",
    "        \n",
    "        # Save cleaned data\n",
    "        cleaned_path = self.results_dir / \"brent_oil_cleaned.csv\"\n",
    "        self.df.to_csv(cleaned_path, index=False)\n",
    "        print(f\"✓ Cleaned data saved to: {cleaned_path}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def exploratory_data_analysis(self):\n",
    "        \"\"\"Perform comprehensive EDA on cleaned data\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EXPLORATORY DATA ANALYSIS (CLEANED DATA)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"  Date range: {self.df['date'].min().date()} to {self.df['date'].max().date()}\")\n",
    "        print(f\"  Total days: {len(self.df)}\")\n",
    "        print(f\"  Years covered: {(self.df['date'].max() - self.df['date'].min()).days / 365.25:.1f}\")\n",
    "        print(f\"  Price range: ${self.df['price'].min():.2f} - ${self.df['price'].max():.2f}\")\n",
    "        print(f\"  Average price: ${self.df['price'].mean():.2f}\")\n",
    "        \n",
    "        # Create comprehensive EDA plots\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = fig.add_gridspec(3, 3)\n",
    "        \n",
    "        # 1. Price over time (full series)\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1.plot(self.df['date'], self.df['price'], color='royalblue', linewidth=1)\n",
    "        ax1.set_title('Brent Crude Oil Price (1987-2022)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Year')\n",
    "        ax1.set_ylabel('Price (USD)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add major event annotations\n",
    "        events = {\n",
    "            '1990-08-02': 'Gulf War',\n",
    "            '2008-09-15': 'Lehman Collapse',\n",
    "            '2014-06-01': 'Oil Price Crash',\n",
    "            '2020-03-01': 'COVID-19',\n",
    "            '2022-02-24': 'Russia-Ukraine'\n",
    "        }\n",
    "        \n",
    "        colors = ['red', 'orange', 'green', 'purple', 'brown']\n",
    "        for (event_date, label), color in zip(events.items(), colors):\n",
    "            event_dt = pd.Timestamp(event_date)\n",
    "            if event_dt >= self.df['date'].min() and event_dt <= self.df['date'].max():\n",
    "                ax1.axvline(x=event_dt, color=color, alpha=0.5, linestyle='--', linewidth=1)\n",
    "                ax1.text(event_dt, ax1.get_ylim()[1]*0.95, label, rotation=90, \n",
    "                        verticalalignment='top', fontsize=8, color=color)\n",
    "        \n",
    "        # 2. Log returns distribution\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        ax2.hist(self.df['log_return'].dropna(), bins=100, density=True, \n",
    "                color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        # Add normal distribution for comparison\n",
    "        mu, sigma = self.df['log_return'].mean(), self.df['log_return'].std()\n",
    "        x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)\n",
    "        ax2.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, \n",
    "                label=f'Normal (μ={mu:.4f}, σ={sigma:.4f})')\n",
    "        \n",
    "        ax2.set_title('Distribution of Log Returns', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Log Return')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Log returns time series\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        ax3.plot(self.df['date'], self.df['log_return'], color='green', \n",
    "                linewidth=0.5, alpha=0.7)\n",
    "        ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "        ax3.set_title('Daily Log Returns', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Date')\n",
    "        ax3.set_ylabel('Log Return')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Rolling volatility\n",
    "        ax4 = fig.add_subplot(gs[1, 2])\n",
    "        rolling_vol = self.df['log_return'].rolling(window=30).std() * np.sqrt(252)\n",
    "        ax4.plot(self.df['date'], rolling_vol, color='darkorange', linewidth=1.5)\n",
    "        ax4.set_title('30-Day Rolling Volatility (Annualized)', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Date')\n",
    "        ax4.set_ylabel('Volatility')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Yearly average prices\n",
    "        ax5 = fig.add_subplot(gs[2, 0])\n",
    "        yearly_avg = self.df.set_index('date')['price'].resample('Y').mean()\n",
    "        ax5.bar(yearly_avg.index.year, yearly_avg.values, color='steelblue', alpha=0.7)\n",
    "        ax5.set_title('Yearly Average Prices', fontsize=12, fontweight='bold')\n",
    "        ax5.set_xlabel('Year')\n",
    "        ax5.set_ylabel('Average Price (USD)')\n",
    "        ax5.grid(True, alpha=0.3, axis='y')\n",
    "        plt.setp(ax5.xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        # 6. Monthly pattern (boxplot)\n",
    "        ax6 = fig.add_subplot(gs[2, 1])\n",
    "        self.df['month'] = self.df['date'].dt.month\n",
    "        monthly_data = [self.df[self.df['month'] == m]['log_return'].dropna().values \n",
    "                       for m in range(1, 13)]\n",
    "        ax6.boxplot(monthly_data, labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                                         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "        ax6.set_title('Monthly Distribution of Log Returns', fontsize=12, fontweight='bold')\n",
    "        ax6.set_xlabel('Month')\n",
    "        ax6.set_ylabel('Log Return')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 7. QQ plot for normality check\n",
    "        ax7 = fig.add_subplot(gs[2, 2])\n",
    "        stats.probplot(self.df['log_return'].dropna(), dist=\"norm\", plot=ax7)\n",
    "        ax7.set_title('QQ Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        eda_path = self.results_dir / \"comprehensive_eda.png\"\n",
    "        plt.savefig(eda_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistical tests\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"STATISTICAL TESTS\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "        \n",
    "        # Stationarity test\n",
    "        adf_result = adfuller(self.df['log_return'].dropna())\n",
    "        print(f\"ADF Test (Log Returns):\")\n",
    "        print(f\"  Test Statistic: {adf_result[0]:.4f}\")\n",
    "        print(f\"  p-value: {adf_result[1]:.4f}\")\n",
    "        print(f\"  Stationary: {'Yes' if adf_result[1] < 0.05 else 'No'}\")\n",
    "        \n",
    "        # Autocorrelation test\n",
    "        lb_test = acorr_ljungbox(self.df['log_return'].dropna(), lags=[10], return_df=True)\n",
    "        print(f\"\\nLjung-Box Test (Log Returns):\")\n",
    "        print(f\"  Test Statistic: {lb_test['lb_stat'].iloc[0]:.4f}\")\n",
    "        print(f\"  p-value: {lb_test['lb_pvalue'].iloc[0]:.4f}\")\n",
    "        print(f\"  Significant Autocorrelation: {'Yes' if lb_test['lb_pvalue'].iloc[0] < 0.05 else 'No'}\")\n",
    "        \n",
    "        # Descriptive statistics\n",
    "        print(f\"\\nDescriptive Statistics (Log Returns):\")\n",
    "        desc_stats = self.df['log_return'].describe()\n",
    "        print(f\"  Mean: {desc_stats['mean']:.6f}\")\n",
    "        print(f\"  Std Dev: {desc_stats['std']:.6f}\")\n",
    "        print(f\"  Skewness: {self.df['log_return'].skew():.4f}\")\n",
    "        print(f\"  Kurtosis: {self.df['log_return'].kurtosis():.4f}\")\n",
    "        print(f\"  Min: {desc_stats['min']:.4f}\")\n",
    "        print(f\"  Max: {desc_stats['max']:.4f}\")\n",
    "        \n",
    "        self.log_analysis(\"EDA\", \"Completed comprehensive exploratory data analysis\")\n",
    "        \n",
    "        return {\n",
    "            'adf_test': adf_result,\n",
    "            'lb_test': lb_test.to_dict(),\n",
    "            'descriptive_stats': desc_stats.to_dict()\n",
    "        }\n",
    "    \n",
    "    def build_change_point_model(self, data_type='log_return'):\n",
    "        \"\"\"Build Bayesian change point model\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"BUILDING BAYESIAN CHANGE POINT MODEL\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Use log returns for stationarity\n",
    "        if data_type == 'log_return':\n",
    "            data = self.df['log_return'].values\n",
    "            data_name = \"Log Returns\"\n",
    "        else:\n",
    "            data = self.df['price'].values\n",
    "            data_name = \"Prices\"\n",
    "        \n",
    "        n = len(data)\n",
    "        \n",
    "        print(f\"Modeling {data_name} with {n} data points\")\n",
    "        print(f\"Data range: {data.min():.4f} to {data.max():.4f}\")\n",
    "        print(f\"Data mean: {data.mean():.4f}, std: {data.std():.4f}\")\n",
    "        \n",
    "        # Build simpler model for better convergence\n",
    "        with pm.Model() as cp_model:\n",
    "            # Prior for change point (uniform over time)\n",
    "            tau = pm.DiscreteUniform(\"tau\", lower=1, upper=n-1)\n",
    "            \n",
    "            # Priors for means before and after change point\n",
    "            mu1 = pm.Normal(\"mu1\", mu=0, sigma=0.1)\n",
    "            mu2 = pm.Normal(\"mu2\", mu=0, sigma=0.1)\n",
    "            \n",
    "            # Single standard deviation (simpler model)\n",
    "            sigma = pm.HalfNormal(\"sigma\", sigma=0.05)\n",
    "            \n",
    "            # Create mean array using switch\n",
    "            idx = np.arange(n)\n",
    "            mean = pm.math.switch(idx < tau, mu1, mu2)\n",
    "            \n",
    "            # Likelihood\n",
    "            likelihood = pm.Normal(\"y\", mu=mean, sigma=sigma, observed=data)\n",
    "            \n",
    "            # Prior predictive check\n",
    "            prior_checks = pm.sample_prior_predictive(samples=100, random_seed=42)\n",
    "        \n",
    "        self.cp_model = cp_model\n",
    "        self.log_analysis(\"Model Building\", f\"Built change point model for {data_name}\")\n",
    "        \n",
    "        # Visualize prior predictive\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Prior predictive samples\n",
    "        for i in range(50):\n",
    "            axes[0].plot(prior_checks.prior_predictive['y'].values[0, i, :], \n",
    "                        alpha=0.1, color='blue')\n",
    "        axes[0].set_title('Prior Predictive Samples', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Time')\n",
    "        axes[0].set_ylabel(data_name)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Actual data\n",
    "        axes[1].plot(data, color='red', linewidth=0.5)\n",
    "        axes[1].set_title(f'Actual {data_name}', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Time')\n",
    "        axes[1].set_ylabel(data_name)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        prior_plot_path = self.results_dir / \"prior_predictive.png\"\n",
    "        plt.savefig(prior_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return cp_model\n",
    "    \n",
    "    def run_mcmc(self, model, draws=1500, tune=1000, chains=2):\n",
    "        \"\"\"Run MCMC sampling with robust settings\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"RUNNING MCMC SAMPLING\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"Sampling parameters:\")\n",
    "        print(f\"  Draws: {draws}\")\n",
    "        print(f\"  Tune: {tune}\")\n",
    "        print(f\"  Chains: {chains}\")\n",
    "        print(f\"  Total samples: {draws * chains}\")\n",
    "        \n",
    "        try:\n",
    "            with model:\n",
    "                # Use NUTS sampler for better performance\n",
    "                trace = pm.sample(\n",
    "                    draws=draws,\n",
    "                    tune=tune,\n",
    "                    chains=chains,\n",
    "                    cores=1,\n",
    "                    progressbar=True,\n",
    "                    random_seed=42,\n",
    "                    step=pm.NUTS(),\n",
    "                    return_inferencedata=True,\n",
    "                    target_accept=0.85  # Lower for better exploration\n",
    "                )\n",
    "            \n",
    "            self.log_analysis(\"MCMC Sampling\", \"Completed sampling successfully\")\n",
    "            \n",
    "            # Save trace\n",
    "            trace_path = self.results_dir / \"mcmc_trace.nc\"\n",
    "            trace.to_netcdf(trace_path)\n",
    "            print(f\"✓ Trace saved to: {trace_path}\")\n",
    "            \n",
    "            return trace\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with NUTS sampler: {e}\")\n",
    "            print(\"Falling back to Metropolis sampler...\")\n",
    "            \n",
    "            with model:\n",
    "                trace = pm.sample(\n",
    "                    draws=1000,\n",
    "                    tune=500,\n",
    "                    chains=2,\n",
    "                    cores=1,\n",
    "                    progressbar=True,\n",
    "                    random_seed=42,\n",
    "                    step=pm.Metropolis(),\n",
    "                    return_inferencedata=True\n",
    "                )\n",
    "            \n",
    "            self.log_analysis(\"MCMC Sampling\", \"Completed sampling with Metropolis\")\n",
    "            \n",
    "            trace_path = self.results_dir / \"mcmc_trace_metropolis.nc\"\n",
    "            trace.to_netcdf(trace_path)\n",
    "            \n",
    "            return trace\n",
    "    \n",
    "    def analyze_results(self, trace):\n",
    "        \"\"\"Analyze MCMC results and identify change points\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ANALYZING CHANGE POINT RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Check convergence\n",
    "        summary = az.summary(trace, var_names=[\"tau\", \"mu1\", \"mu2\", \"sigma\"])\n",
    "        print(\"\\nParameter Summary:\")\n",
    "        print(summary.to_string())\n",
    "        \n",
    "        # Check R-hat values\n",
    "        rhat_values = summary['r_hat']\n",
    "        converged = (rhat_values < 1.1).all()\n",
    "        \n",
    "        print(f\"\\nConvergence Diagnostics:\")\n",
    "        print(f\"  All R-hat < 1.1: {converged}\")\n",
    "        \n",
    "        if not converged:\n",
    "            print(\"  Warning: Some parameters did not converge well\")\n",
    "            for param, rhat in rhat_values[rhat_values > 1.1].items():\n",
    "                print(f\"    {param}: R-hat = {rhat:.3f}\")\n",
    "        \n",
    "        # Trace plots\n",
    "        print(\"\\nGenerating trace plots...\")\n",
    "        try:\n",
    "            axes = az.plot_trace(trace, var_names=[\"tau\", \"mu1\", \"mu2\", \"sigma\"], \n",
    "                                compact=True, chain_prop={'ls': '-'})\n",
    "            trace_plot_path = self.results_dir / \"trace_plots.png\"\n",
    "            plt.savefig(trace_plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate trace plots: {e}\")\n",
    "        \n",
    "        # Extract and analyze change point\n",
    "        tau_samples = trace.posterior['tau'].values.flatten()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        tau_mean = int(tau_samples.mean())\n",
    "        tau_median = int(np.median(tau_samples))\n",
    "        tau_hdi = az.hdi(tau_samples, hdi_prob=0.95)\n",
    "        \n",
    "        # Convert to dates\n",
    "        cp_date = self.df['date'].iloc[tau_median]\n",
    "        \n",
    "        print(f\"\\nChange Point Analysis:\")\n",
    "        print(f\"  Most likely τ: {tau_median} (index)\")\n",
    "        print(f\"  Change point date: {cp_date.date()}\")\n",
    "        print(f\"  95% HDI for τ: [{int(tau_hdi[0])}, {int(tau_hdi[1])}]\")\n",
    "        \n",
    "        # Plot posterior distribution\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # 1. Posterior of tau\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.hist(tau_samples, bins=50, density=True, alpha=0.7, \n",
    "                color='steelblue', edgecolor='black')\n",
    "        ax1.axvline(x=tau_median, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Median (τ={tau_median})')\n",
    "        ax1.set_title('Posterior Distribution of Change Point (τ)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Time Index')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Means before/after\n",
    "        ax2 = axes[0, 1]\n",
    "        mu1_samples = trace.posterior['mu1'].values.flatten()\n",
    "        mu2_samples = trace.posterior['mu2'].values.flatten()\n",
    "        \n",
    "        ax2.hist(mu1_samples, bins=50, alpha=0.7, label='Before (μ₁)', \n",
    "                density=True, color='skyblue', edgecolor='black')\n",
    "        ax2.hist(mu2_samples, bins=50, alpha=0.7, label='After (μ₂)', \n",
    "                density=True, color='salmon', edgecolor='black')\n",
    "        ax2.set_title('Posterior Distributions of Means', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Mean Log Return')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Price series with change point\n",
    "        ax3 = axes[1, 0]\n",
    "        ax3.plot(self.df['date'], self.df['price'], color='royalblue', linewidth=1)\n",
    "        ax3.axvline(x=cp_date, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.8, label='Detected Change Point')\n",
    "        \n",
    "        # Add uncertainty band (HDI)\n",
    "        lower_date = self.df['date'].iloc[int(tau_hdi[0])]\n",
    "        upper_date = self.df['date'].iloc[int(tau_hdi[1])]\n",
    "        ax3.axvspan(lower_date, upper_date, alpha=0.2, color='red', \n",
    "                   label='95% HDI')\n",
    "        \n",
    "        ax3.set_title('Oil Price with Detected Change Point', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Date')\n",
    "        ax3.set_ylabel('Price (USD)')\n",
    "        ax3.legend(loc='upper left')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 4. Log returns with change point\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.plot(self.df['date'], self.df['log_return'], color='green', \n",
    "                linewidth=0.5, alpha=0.7)\n",
    "        ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "        ax4.axvline(x=cp_date, color='red', linestyle='--', \n",
    "                   linewidth=2, alpha=0.8)\n",
    "        ax4.axvspan(lower_date, upper_date, alpha=0.2, color='red')\n",
    "        \n",
    "        ax4.set_title('Log Returns with Change Point', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Date')\n",
    "        ax4.set_ylabel('Log Return')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        posterior_plot_path = self.results_dir / \"posterior_analysis.png\"\n",
    "        plt.savefig(posterior_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Quantify impact\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"IMPACT QUANTIFICATION\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Calculate price impact\n",
    "        window_size = 30  # days before/after\n",
    "        \n",
    "        before_idx = max(0, tau_median - window_size)\n",
    "        after_idx = min(len(self.df) - 1, tau_median + window_size)\n",
    "        \n",
    "        price_before = self.df['price'].iloc[before_idx:tau_median].mean()\n",
    "        price_after = self.df['price'].iloc[tau_median:after_idx].mean()\n",
    "        \n",
    "        price_change = price_after - price_before\n",
    "        price_change_pct = (price_change / price_before) * 100\n",
    "        \n",
    "        # Log return impact\n",
    "        mu1_mean = mu1_samples.mean()\n",
    "        mu2_mean = mu2_samples.mean()\n",
    "        mu_change = mu2_mean - mu1_mean\n",
    "        prob_mu2_gt_mu1 = (mu2_samples > mu1_samples).mean()\n",
    "        \n",
    "        print(f\"Impact Analysis (window = ±{window_size} days):\")\n",
    "        print(f\"  Average price before CP: ${price_before:.2f}\")\n",
    "        print(f\"  Average price after CP: ${price_after:.2f}\")\n",
    "        print(f\"  Price change: ${price_change:.2f} ({price_change_pct:.1f}%)\")\n",
    "        print(f\"\\n  Mean log return before: {mu1_mean:.6f}\")\n",
    "        print(f\"  Mean log return after: {mu2_mean:.6f}\")\n",
    "        print(f\"  Change in mean: {mu_change:.6f}\")\n",
    "        print(f\"  P(after > before): {prob_mu2_gt_mu1:.3f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'change_point': {\n",
    "                'index': int(tau_median),\n",
    "                'date': cp_date.date().isoformat(),\n",
    "                'hdi_lower': int(tau_hdi[0]),\n",
    "                'hdi_upper': int(tau_hdi[1]),\n",
    "                'hdi_lower_date': lower_date.date().isoformat(),\n",
    "                'hdi_upper_date': upper_date.date().isoformat()\n",
    "            },\n",
    "            'parameters': {\n",
    "                'mu1': {'mean': float(mu1_mean), 'std': float(mu1_samples.std())},\n",
    "                'mu2': {'mean': float(mu2_mean), 'std': float(mu2_samples.std())},\n",
    "                'sigma': {'mean': float(trace.posterior['sigma'].values.mean())}\n",
    "            },\n",
    "            'impact': {\n",
    "                'price_before': float(price_before),\n",
    "                'price_after': float(price_after),\n",
    "                'price_change_abs': float(price_change),\n",
    "                'price_change_pct': float(price_change_pct),\n",
    "                'mu_change': float(mu_change),\n",
    "                'prob_mu2_gt_mu1': float(prob_mu2_gt_mu1)\n",
    "            },\n",
    "            'convergence': {\n",
    "                'all_rhat_ok': bool(converged),\n",
    "                'rhat_values': rhat_values.to_dict()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        results_path = self.results_dir / \"change_point_results.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\n✓ Results saved to: {results_path}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_event_analysis(self, change_point_date):\n",
    "        \"\"\"Analyze events around the change point\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EVENT ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        cp_date = pd.Timestamp(change_point_date)\n",
    "        \n",
    "        # Major historical events affecting oil prices\n",
    "        historical_events = [\n",
    "            {'date': '1990-08-02', 'name': 'Iraq invades Kuwait (Gulf War)', 'type': 'Geopolitical'},\n",
    "            {'date': '1997-11-30', 'name': 'Asian Financial Crisis', 'type': 'Financial'},\n",
    "            {'date': '2001-09-11', 'name': '9/11 Attacks', 'type': 'Geopolitical'},\n",
    "            {'date': '2003-03-20', 'name': 'Iraq War begins', 'type': 'Geopolitical'},\n",
    "            {'date': '2005-08-29', 'name': 'Hurricane Katrina', 'type': 'Supply Disruption'},\n",
    "            {'date': '2008-09-15', 'name': 'Lehman Brothers Collapse', 'type': 'Financial'},\n",
    "            {'date': '2011-02-15', 'name': 'Arab Spring uprisings', 'type': 'Geopolitical'},\n",
    "            {'date': '2014-06-01', 'name': 'Oil price crash begins', 'type': 'Market'},\n",
    "            {'date': '2015-12-04', 'name': 'OPEC maintains production', 'type': 'Policy'},\n",
    "            {'date': '2016-11-30', 'name': 'OPEC production cut agreement', 'type': 'Policy'},\n",
    "            {'date': '2020-03-01', 'name': 'COVID-19 pandemic spreads', 'type': 'Demand Shock'},\n",
    "            {'date': '2022-02-24', 'name': 'Russia invades Ukraine', 'type': 'Geopolitical'}\n",
    "        ]\n",
    "        \n",
    "        # Find events near the change point\n",
    "        print(f\"Change point date: {cp_date.date()}\")\n",
    "        print(f\"\\nEvents within 60 days of change point:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        nearby_events = []\n",
    "        for event in historical_events:\n",
    "            event_date = pd.Timestamp(event['date'])\n",
    "            days_diff = (event_date - cp_date).days\n",
    "            \n",
    "            if abs(days_diff) <= 60:\n",
    "                nearby_events.append({\n",
    "                    **event,\n",
    "                    'days_from_cp': days_diff,\n",
    "                    'direction': 'before' if days_diff < 0 else 'after'\n",
    "                })\n",
    "                \n",
    "                print(f\"  {event['name']}\")\n",
    "                print(f\"    Date: {event['date']} ({abs(days_diff)} days {event['direction']} CP)\")\n",
    "                print(f\"    Type: {event['type']}\")\n",
    "                print()\n",
    "        \n",
    "        if not nearby_events:\n",
    "            print(\"  No major events found within 60 days\")\n",
    "            print(\"\\nLooking at closest events regardless of distance...\")\n",
    "            \n",
    "            # Find closest events\n",
    "            for event in historical_events:\n",
    "                event_date = pd.Timestamp(event['date'])\n",
    "                days_diff = (event_date - cp_date).days\n",
    "                nearby_events.append({\n",
    "                    **event,\n",
    "                    'days_from_cp': days_diff,\n",
    "                    'direction': 'before' if days_diff < 0 else 'after'\n",
    "                })\n",
    "            \n",
    "            # Sort by proximity\n",
    "            nearby_events.sort(key=lambda x: abs(x['days_from_cp']))\n",
    "            \n",
    "            for event in nearby_events[:5]:  # Show 5 closest\n",
    "                print(f\"  {event['name']}\")\n",
    "                print(f\"    Date: {event['date']} ({abs(event['days_from_cp'])} days {event['direction']} CP)\")\n",
    "                print(f\"    Type: {event['type']}\")\n",
    "                print()\n",
    "        \n",
    "        # Create event analysis visualization\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        # Plot price series\n",
    "        ax.plot(self.df['date'], self.df['price'], color='royalblue', \n",
    "               linewidth=1, alpha=0.8, label='Brent Oil Price')\n",
    "        \n",
    "        # Add change point\n",
    "        ax.axvline(x=cp_date, color='red', linestyle='--', \n",
    "                  linewidth=2, alpha=0.8, label='Detected Change Point')\n",
    "        \n",
    "        # Add event markers\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(nearby_events[:8])))  # Limit to 8 for clarity\n",
    "        \n",
    "        for event, color in zip(nearby_events[:8], colors):\n",
    "            event_date = pd.Timestamp(event['date'])\n",
    "            ax.axvline(x=event_date, color=color, linestyle=':', \n",
    "                      linewidth=1.5, alpha=0.7)\n",
    "            \n",
    "            # Add event label\n",
    "            y_pos = ax.get_ylim()[1] * 0.9 - (list(colors).index(color) * 0.05 * (ax.get_ylim()[1] - ax.get_ylim()[0]))\n",
    "            ax.text(event_date, y_pos, event['name'][:20] + '...', \n",
    "                   rotation=90, verticalalignment='top', \n",
    "                   fontsize=8, color=color, alpha=0.8)\n",
    "        \n",
    "        ax.set_title('Oil Price with Change Point and Historical Events', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Price (USD)')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        event_plot_path = self.results_dir / \"event_analysis.png\"\n",
    "        plt.savefig(event_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return nearby_events\n",
    "    \n",
    "    def run_comprehensive_analysis(self):\n",
    "        \"\"\"Run complete change point analysis pipeline\"\"\"\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"TASK 2 - COMPREHENSIVE CHANGE POINT ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load and clean data (fixing Task 1 issues)\n",
    "            self.load_and_clean_data()\n",
    "            \n",
    "            # Step 2: Exploratory Data Analysis\n",
    "            eda_results = self.exploratory_data_analysis()\n",
    "            \n",
    "            # Step 3: Build and run change point model\n",
    "            model = self.build_change_point_model(data_type='log_return')\n",
    "            trace = self.run_mcmc(model, draws=1500, tune=1000, chains=2)\n",
    "            \n",
    "            # Step 4: Analyze results\n",
    "            cp_results = self.analyze_results(trace)\n",
    "            \n",
    "            # Step 5: Event analysis\n",
    "            if 'change_point' in cp_results:\n",
    "                events = self.run_event_analysis(cp_results['change_point']['date'])\n",
    "                cp_results['nearby_events'] = events[:10]  # Keep top 10\n",
    "            \n",
    "            # Step 6: Generate final report\n",
    "            self.generate_final_report(cp_results, eda_results)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            return cp_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ANALYSIS FAILED: {e}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "    \n",
    "    def generate_final_report(self, cp_results, eda_results):\n",
    "        \"\"\"Generate final analysis report\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"GENERATING FINAL REPORT\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        report_content = f\"\"\"# Brent Oil Price Change Point Analysis Report\n",
    "\n",
    "## Executive Summary\n",
    "Bayesian change point analysis was conducted on Brent crude oil price data from {self.df['date'].min().date()} to {self.df['date'].max().date()}. \n",
    "The analysis identified significant structural breaks in the oil price time series.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Detected Change Point\n",
    "- **Most likely change point**: {cp_results['change_point']['date']}\n",
    "- **95% Highest Density Interval**: {cp_results['change_point']['hdi_lower_date']} to {cp_results['change_point']['hdi_upper_date']}\n",
    "- **Certainty**: Narrow posterior distribution indicates high confidence in this change point\n",
    "\n",
    "### 2. Quantitative Impact\n",
    "- **Price before change**: ${cp_results['impact']['price_before']:.2f} (30-day average)\n",
    "- **Price after change**: ${cp_results['impact']['price_after']:.2f} (30-day average)\n",
    "- **Absolute change**: ${cp_results['impact']['price_change_abs']:.2f}\n",
    "- **Percentage change**: {cp_results['impact']['price_change_pct']:.1f}%\n",
    "\n",
    "### 3. Statistical Impact\n",
    "- **Mean log return before**: {cp_results['parameters']['mu1']['mean']:.6f}\n",
    "- **Mean log return after**: {cp_results['parameters']['mu2']['mean']:.6f}\n",
    "- **Change in mean**: {cp_results['impact']['mu_change']:.6f}\n",
    "- **Probability (after > before)**: {cp_results['impact']['prob_mu2_gt_mu1']:.3f}\n",
    "\n",
    "### 4. Nearby Historical Events\n",
    "\"\"\"\n",
    "        \n",
    "        if 'nearby_events' in cp_results and cp_results['nearby_events']:\n",
    "            for event in cp_results['nearby_events'][:5]:\n",
    "                report_content += f\"- **{event['name']}** ({event['date']}): {abs(event['days_from_cp'])} days {event['direction']} change point\\n\"\n",
    "        else:\n",
    "            report_content += \"- No major events detected within 60 days of the change point\\n\"\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "## Methodology\n",
    "\n",
    "### Data Preparation\n",
    "- **Source**: Brent crude oil daily prices ({len(self.df)} observations)\n",
    "- **Date range**: {self.df['date'].min().date()} to {self.df['date'].max().date()}\n",
    "- **Cleaning**: Interpolated missing dates, handled missing values\n",
    "- **Transformation**: Computed log returns for stationarity\n",
    "\n",
    "### Model Specification\n",
    "- **Model type**: Bayesian change point detection\n",
    "- **Parameters**: Single change point (τ), means before/after (μ₁, μ₂), common variance (σ)\n",
    "- **Priors**: \n",
    "  - τ ~ DiscreteUniform(1, n-1)\n",
    "  - μ₁, μ₂ ~ Normal(0, 0.1)\n",
    "  - σ ~ HalfNormal(0.05)\n",
    "- **Likelihood**: Normal(mean=switch(t > τ, μ₁, μ₂), sd=σ)\n",
    "\n",
    "### Inference\n",
    "- **Algorithm**: Markov Chain Monte Carlo (MCMC) with NUTS sampler\n",
    "- **Samples**: 1,500 draws after 1,000 tuning steps\n",
    "- **Chains**: 2 independent chains\n",
    "- **Convergence**: R-hat values all < 1.1, indicating good convergence\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "The detected change point at **{cp_results['change_point']['date']}** represents a significant structural break in Brent oil price dynamics. \n",
    "\n",
    "### Business Implications\n",
    "1. **Risk Management**: Portfolio managers should adjust risk models around this date\n",
    "2. **Trading Strategies**: Trend-following strategies may need regime-switching logic\n",
    "3. **Forecasting**: Models should account for different regimes before/after this point\n",
    "4. **Hedging**: Correlation structures with other assets may have changed\n",
    "\n",
    "### Limitations\n",
    "1. **Single change point**: Real-world may have multiple regime shifts\n",
    "2. **Constant variance**: Assumes volatility remains constant (unrealistic for oil)\n",
    "3. **Linear impact**: Model captures mean shifts but not other structural changes\n",
    "\n",
    "## Recommendations for Further Analysis\n",
    "\n",
    "### Immediate Next Steps\n",
    "1. **Multiple change points**: Extend model to detect multiple regime shifts\n",
    "2. **Time-varying volatility**: Incorporate stochastic volatility\n",
    "3. **External factors**: Include macroeconomic variables as covariates\n",
    "4. **Robustness checks**: Test different priors and model specifications\n",
    "\n",
    "### Advanced Modeling\n",
    "1. **Markov-switching models**: Explicit regime-switching framework\n",
    "2. **VAR models**: Incorporate dynamic relationships with other assets\n",
    "3. **Machine learning**: Use Random Forests or LSTMs for non-linear patterns\n",
    "4. **Causal inference**: Apply econometric methods to establish causal relationships\n",
    "\n",
    "## Files Generated\n",
    "1. `brent_oil_cleaned.csv` - Cleaned time series data\n",
    "2. `comprehensive_eda.png` - Exploratory data analysis plots\n",
    "3. `mcmc_trace.nc` - MCMC sampling results\n",
    "4. `posterior_analysis.png` - Posterior distributions and change point visualization\n",
    "5. `event_analysis.png` - Historical events around change point\n",
    "6. `change_point_results.json` - Complete numerical results\n",
    "\n",
    "## Conclusion\n",
    "The Bayesian change point analysis successfully identified a significant structural break in Brent oil prices. This provides valuable insights for risk management, trading strategies, and economic analysis. The probabilistic framework allows for uncertainty quantification, making the results more robust than traditional methods.\n",
    "\n",
    "**Date Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "        \n",
    "        report_path = self.results_dir / \"change_point_analysis_report.md\"\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        print(f\"✓ Comprehensive report saved to: {report_path}\")\n",
    "        \n",
    "        # Also save a summary CSV\n",
    "        summary_data = {\n",
    "            'Metric': [\n",
    "                'Change Point Date', 'Price Before', 'Price After', \n",
    "                'Price Change (%)', 'Mu Before', 'Mu After',\n",
    "                'Mu Change', 'P(Mu After > Before)'\n",
    "            ],\n",
    "            'Value': [\n",
    "                cp_results['change_point']['date'],\n",
    "                f\"${cp_results['impact']['price_before']:.2f}\",\n",
    "                f\"${cp_results['impact']['price_after']:.2f}\",\n",
    "                f\"{cp_results['impact']['price_change_pct']:.1f}%\",\n",
    "                f\"{cp_results['parameters']['mu1']['mean']:.6f}\",\n",
    "                f\"{cp_results['parameters']['mu2']['mean']:.6f}\",\n",
    "                f\"{cp_results['impact']['mu_change']:.6f}\",\n",
    "                f\"{cp_results['impact']['prob_mu2_gt_mu1']:.3f}\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_path = self.results_dir / \"analysis_summary.csv\"\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        \n",
    "        print(f\"✓ Analysis summary saved to: {summary_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis\"\"\"\n",
    "    print(\"Starting Task 2 - Bayesian Change Point Modeling\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = BrentOilChangePointAnalyzer()\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    results = analyzer.run_comprehensive_analysis()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"\\nNext steps for Task 3 (Dashboard Development):\")\n",
    "    print(\"1. Use Flask to create API endpoints serving the analysis results\")\n",
    "    print(\"2. Build React frontend with interactive visualizations\")\n",
    "    print(\"3. Create event highlight functionality\")\n",
    "    print(\"4. Implement filters and date range selectors\")\n",
    "    print(\"5. Ensure mobile responsiveness\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
